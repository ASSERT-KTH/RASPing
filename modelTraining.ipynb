{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# The default of float16 can lead to discrepancies between outputs of\n",
    "# the compiled model and the RASP program.\n",
    "jax.config.update('jax_default_matmul_precision', 'float32')\n",
    "\n",
    "from tracr.compiler import compiling\n",
    "from tracr.compiler import lib\n",
    "from tracr.rasp import rasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, raspFunction: rasp.SOp, inputs, seqLength: int, name: str):\n",
    "        self.raspFunction = raspFunction\n",
    "        self.inputs = inputs\n",
    "        self.seqLength = seqLength\n",
    "        self.model = compiling.compile_rasp_to_model(self.raspFunction, self.inputs, self.seqLength, compiler_bos=\"BOS\")\n",
    "        self.name = name\n",
    "    \n",
    "    def evaluate(self, input):\n",
    "        return self.model.apply(input).decoded\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "inputs = [0,1,2,3,4,5]\n",
    "print(inputs[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['BOS', '{', '}'], ['BOS', 1, 1]), (['BOS', '(', ')', '}', '{'], ['BOS', 0, 0, 0, 0]), (['BOS', '}', '}', '{', '('], ['BOS', 0, 0, 0, 0]), (['BOS', '(', ')', '{', '}', '{'], ['BOS', 0, 0, 0, 0, 0]), (['BOS', '(', ')', '(', ')'], ['BOS', 1, 1, 1, 1])]\n"
     ]
    }
   ],
   "source": [
    "acceptedNamesAndInput = {\"reverse\": [\"a\",\"b\",\"c\",\"d\",\"e\"], #Tokens doesn't matter much. Only the quantity influnce the results due to encoding (I think)\n",
    "                         \"hist\": [\"a\",\"b\",\"c\",\"d\"], #Tokens doesn't matter much. Only the quantity influnce the results due to encoding (I think)\n",
    "                         \"sort\": [1,2,3,4,5,6], #[0,1,2,3,4,5,6]    Seems to fail sometimes if 0 is included (irrespktive of if 0 is in the failed input or not, don't know why)\n",
    "                         \"most-freq\": [1,2,3,4,5],\n",
    "                         \"shuffle_dyck1\": [\"(\",\")\"],\n",
    "                         \"shuffle_dyck2\": [\"(\",\")\",\"{\",\"}\"]}     #Could theoretically be adapted into shuffle dyck-k but would still require unique tokens for each k\n",
    "\n",
    "def generateData(name: str, maxSeqLength: int, size: int):\n",
    "    data = [None]*size\n",
    "\n",
    "    match name:\n",
    "        case \"reverse\":\n",
    "            acceptedTokens = acceptedNamesAndInput[name]\n",
    "\n",
    "            for i in range(size):\n",
    "                inputLength = np.random.randint(2, maxSeqLength+1)  #Uniformly distributed between 2 and max length\n",
    "\n",
    "                inputSeq = []\n",
    "                outputSeq = []\n",
    "                for t in np.random.choice(acceptedTokens, inputLength):\n",
    "                    inputSeq.append(t)\n",
    "                    outputSeq.insert(0,t)\n",
    "                inputSeq.insert(0,\"BOS\")\n",
    "                outputSeq.insert(0,\"BOS\")\n",
    "\n",
    "                data[i] = (inputSeq, outputSeq)\n",
    "\n",
    "        case \"hist\":\n",
    "            acceptedTokens = acceptedNamesAndInput[name]  \n",
    "            \n",
    "            for i in range(size):\n",
    "                inputLength = np.random.randint(2, maxSeqLength+1)  #Uniformly distributed between 2 and max length\n",
    "\n",
    "                inputSeq = []\n",
    "                tokenCounter = dict(zip(acceptedTokens, [0]*len(acceptedTokens)))   #Counter built during generating input\n",
    "                for t in np.random.choice(acceptedTokens, inputLength):\n",
    "                    inputSeq.append(t)\n",
    "                    tokenCounter[t]+=1\n",
    "    \n",
    "                outputSeq = []\n",
    "                for t in inputSeq:  #Fill output according to token counter\n",
    "                    outputSeq.append(tokenCounter[t])\n",
    "\n",
    "                inputSeq.insert(0,\"BOS\")\n",
    "                outputSeq.insert(0,\"BOS\")\n",
    "\n",
    "                data[i] = (inputSeq, outputSeq)\n",
    "\n",
    "        case \"sort\":\n",
    "            acceptedTokens = acceptedNamesAndInput[name]  \n",
    "            \n",
    "            for i in range(size):\n",
    "                inputLength = np.random.randint(2, maxSeqLength+1)  #Uniformly distributed between 2 and max length\n",
    "\n",
    "                inputSeq = []\n",
    "                outputSeq = []\n",
    "                for t in np.random.choice(acceptedTokens, inputLength):\n",
    "                    inputSeq.append(t)\n",
    "                    outputSeq.append(t)\n",
    "    \n",
    "                inputSeq.insert(0,\"BOS\")\n",
    "                outputSeq.sort()\n",
    "                outputSeq.insert(0,\"BOS\")\n",
    "\n",
    "                data[i] = (inputSeq, outputSeq)\n",
    "\n",
    "        case \"most-freq\":   #sort based on most frequent token with original position as tie breaker\n",
    "            acceptedTokens = acceptedNamesAndInput[name]  \n",
    "\n",
    "            for i in range(size):\n",
    "                inputLength = np.random.randint(2, maxSeqLength+1)  #Uniformly distributed between 2 and max length\n",
    "\n",
    "                inputSeq = []\n",
    "                tempSeq = []\n",
    "                tokenCounter = dict(zip(acceptedTokens, [0]*len(acceptedTokens)))   #Counter built during generating input\n",
    "                for t in np.random.choice(acceptedTokens, inputLength):\n",
    "                    inputSeq.append(t)\n",
    "                    tokenCounter[t]+=1\n",
    "                    tempSeq.append(t)    \n",
    "                \n",
    "                tempSeq.sort(key = (lambda x: -tokenCounter[x]))  #Sort the list in descending order of frequency\n",
    "\n",
    "                outputSeq = tempSeq\n",
    "\n",
    "                #Groups the tokens (Apparently not done by the Tracr solution)\n",
    "                \"\"\"\n",
    "                outputSeq = []\n",
    "                for t in tempSeq:\n",
    "                    if t not in outputSeq:\n",
    "                        for ii in range(tokenCounter[t]):\n",
    "                            outputSeq.append(t)\n",
    "                \"\"\"\n",
    "\n",
    "                inputSeq.insert(0,\"BOS\")\n",
    "                outputSeq.insert(0,\"BOS\")\n",
    "\n",
    "                data[i] = (inputSeq, outputSeq)\n",
    "\n",
    "        case \"shuffle_dyck1\":\n",
    "            acceptedTokens = acceptedNamesAndInput[name]\n",
    "\n",
    "            for i in range(size):\n",
    "                for ii in range(3):     #Ensures that roughly one out of eight sequences has an odd length\n",
    "                    inputLength = np.random.randint(2, maxSeqLength+1)  #Uniformly distributed between 2 and max length\n",
    "                    if inputLength%2==0:\n",
    "                        break\n",
    "\n",
    "                inputSeq = []\n",
    "                tokenCount = {\"(\":0,\")\":0}\n",
    "                tokenProb = np.zeros(len(acceptedTokens))   #Live probabilty distribution to more evenly distribute the balanced and unblanaced sequences\n",
    "                tokenProb[1] = 1/(inputLength+1)\n",
    "                tokenProb[0] = 1 - tokenProb[1]\n",
    "\n",
    "                #Build the sequence token by token and ensuring the probability of drawing a balanced sequence is always higher than drawing an unbalanced sequence\n",
    "                for ind in range(inputLength):\n",
    "                    t = np.random.choice(acceptedTokens, 1, p=tokenProb)[0]\n",
    "                    tokenCount[t]+=1\n",
    "                    inputSeq.append(t)\n",
    "\n",
    "                    tokenDiff = tokenCount[\"(\"]-tokenCount[\")\"]\n",
    "                    if tokenDiff == 0:  #High probability of begining paranthesis if balanced\n",
    "                        tokenProb[1] = 1/(inputLength+1)\n",
    "                        tokenProb[0] = 1 - tokenProb[1]\n",
    "                    elif tokenDiff > 0:   #High probability of end paranthesis if more begining paranthesis\n",
    "                        tokenProb[0] = 1/((inputLength+1)*tokenDiff)\n",
    "                        tokenProb[1] = 1 - tokenProb[0]\n",
    "                    else: #High probability of begining paranthesis if more end paranthesis\n",
    "                        tokenProb[1] = 1/((inputLength+1)*(-tokenDiff))\n",
    "                        tokenProb[0] = 1 - tokenProb[1]\n",
    "                \n",
    "                #Checks for balance\n",
    "                balanceCounter=0\n",
    "                for t in inputSeq:\n",
    "                    if t==\"(\":\n",
    "                        balanceCounter+=1\n",
    "                    else:\n",
    "                        balanceCounter-=1\n",
    "                    if balanceCounter<0:\n",
    "                        break\n",
    "                \n",
    "                if balanceCounter!=0:\n",
    "                    outputSeq = [0]*len(inputSeq)\n",
    "                else:\n",
    "                    outputSeq = [1]*len(inputSeq)\n",
    "\n",
    "                inputSeq.insert(0,\"BOS\")\n",
    "                outputSeq.insert(0,\"BOS\")\n",
    "\n",
    "                data[i] = (inputSeq, outputSeq)\n",
    "\n",
    "        case \"shuffle_dyck2\":\n",
    "            acceptedTokens = acceptedNamesAndInput[name]\n",
    "\n",
    "            for i in range(size):\n",
    "                for ii in range(3):     #Ensures that roughly one out of eight sequences has an odd length\n",
    "                    inputLength = np.random.randint(2, maxSeqLength+1)  #Uniformly distributed between 2 and max length\n",
    "                    if inputLength%2==0:\n",
    "                        break\n",
    "\n",
    "                inputSeq = []\n",
    "                tokenCount = {\"(\":0,\")\":0,\"{\":0,\"}\":0}\n",
    "                tokenProb = np.zeros(len(acceptedTokens))   #Live probabilty distribution to more evenly distribute the balanced and unblanaced sequences\n",
    "                tokenProb[1] = 1/((inputLength+1)*2)\n",
    "                tokenProb[0] = 1/2 - tokenProb[1]\n",
    "                tokenProb[3] = tokenProb[1]\n",
    "                tokenProb[2] = tokenProb[0]\n",
    "\n",
    "                #Build the sequence token by token and ensuring the probability of drawing a balanced sequence is always higher than drawing an unbalanced sequence\n",
    "                for ind in range(inputLength):\n",
    "                    t = np.random.choice(acceptedTokens, 1, p=tokenProb)[0]\n",
    "                    tokenCount[t]+=1\n",
    "                    inputSeq.append(t)\n",
    "\n",
    "                    tokenDiff1 = tokenCount[\"(\"]-tokenCount[\")\"]\n",
    "                    tokenDiff2 = tokenCount[\"{\"]-tokenCount[\"}\"]\n",
    "                    if tokenDiff1 == 0 and tokenDiff2==0:  #High probability of begining paranthesis if balanced\n",
    "                        tokenProb[1] = 1/((inputLength+1)*2)\n",
    "                        tokenProb[0] = 1/2 - tokenProb[1]\n",
    "                        tokenProb[3] = tokenProb[1]\n",
    "                        tokenProb[2] = tokenProb[0]\n",
    "                    #High probability of end paranthesis if more begining paranthesis\n",
    "                    elif tokenDiff2 > 0 and tokenDiff1 > 0:\n",
    "                        tokenProb[0] = 1/((inputLength+1)*tokenDiff1*2)\n",
    "                        tokenProb[2] = 1/((inputLength+1)*tokenDiff2*2)\n",
    "                        tokenProb[1] = 1/2 - tokenProb[0]\n",
    "                        tokenProb[3] = 1/2 - tokenProb[2]\n",
    "                    elif tokenDiff1 > 0 and tokenDiff2==0: \n",
    "                        tokenProb[1] = 1 - 1/((inputLength+1)*tokenDiff1)\n",
    "                        split = 1 - tokenProb[1]    #The reminder of probability to distribute\n",
    "                        tokenProb[2] = split - split/((inputLength+1))    #More likely to start a new parenthesis than break sequence\n",
    "                        split = split - tokenProb[2]\n",
    "                        tokenProb[0] = split/2\n",
    "                        tokenProb[3] = split/2\n",
    "                    elif tokenDiff2 > 0 and tokenDiff1==0:   \n",
    "                        tokenProb[3] = 1 - 1/((inputLength+1)*tokenDiff2)\n",
    "                        split = 1 - tokenProb[3]    #The reminder of probability to distribute\n",
    "                        tokenProb[0] = split - split/((inputLength+1))    #More likely to start a new parenthesis than break sequence\n",
    "                        split = split - tokenProb[0]\n",
    "                        tokenProb[1] = split/2\n",
    "                        tokenProb[2] = split/2\n",
    "                    #High probability of begining paranthesis if more end paranthesis\n",
    "                    elif tokenDiff2 < 0 and tokenDiff1 < 0:\n",
    "                        tokenProb[1] = 1/((inputLength+1)*(-tokenDiff1)*2)\n",
    "                        tokenProb[3] = 1/((inputLength+1)*(-tokenDiff2)*2)\n",
    "                        tokenProb[0] = 1/2 - tokenProb[1]\n",
    "                        tokenProb[2] = 1/2 - tokenProb[3]\n",
    "                    elif tokenDiff1 < 0 and tokenDiff2 == 0:\n",
    "                        tokenProb[0] = 1 - 1/((inputLength+1)*(-tokenDiff1))\n",
    "                        split = 1 - tokenProb[0]    #The reminder of probability to distribute\n",
    "                        tokenProb[2] = split - split/((inputLength+1))    #More likely to start a new parenthesis than break sequence\n",
    "                        split = split - tokenProb[2]\n",
    "                        tokenProb[1] = split/2\n",
    "                        tokenProb[3] = split/2\n",
    "                    elif tokenDiff2 < 0 and tokenDiff1 == 0:\n",
    "                        tokenProb[2] = 1 - 1/((inputLength+1)*(-tokenDiff2))\n",
    "                        split = 1 - tokenProb[2]    #The reminder of probability to distribute\n",
    "                        tokenProb[0] = split - split/((inputLength+1))    #More likely to start a new parenthesis than break sequence\n",
    "                        split = split - tokenProb[0]\n",
    "                        tokenProb[1] = split/2\n",
    "                        tokenProb[3] = split/2\n",
    "                    #Higher probability to balance the sequence if currently unbalanced\n",
    "                    elif tokenDiff1 > 0 and tokenDiff2 < 0:\n",
    "                        tokenProb[1] = 1/((inputLength+1)*tokenDiff1*2)\n",
    "                        tokenProb[2] = 1/((inputLength+1)*(-tokenDiff2)*2)\n",
    "                        tokenProb[0] = 1/2 - tokenProb[1]\n",
    "                        tokenProb[3] = 1/2 - tokenProb[2]\n",
    "                    elif tokenDiff2 > 0 and tokenDiff1 < 0:\n",
    "                        tokenProb[3] = 1/((inputLength+1)*tokenDiff2*2)\n",
    "                        tokenProb[0] = 1/((inputLength+1)*(-tokenDiff1)*2)\n",
    "                        tokenProb[1] = 1/2 - tokenProb[0]\n",
    "                        tokenProb[2] = 1/2 - tokenProb[3]\n",
    "                \n",
    "                #Checks for balance\n",
    "                balanceCounter=[0,0]\n",
    "                for t in inputSeq:\n",
    "                    if t==\"(\":\n",
    "                        balanceCounter[0]+=1\n",
    "                    if t==\")\":\n",
    "                        balanceCounter[0]-=1\n",
    "                    if t==\"{\":\n",
    "                        balanceCounter[1]+=1\n",
    "                    if t==\"}\":\n",
    "                        balanceCounter[1]-=1\n",
    "                    \n",
    "                    if balanceCounter[0]<0 or balanceCounter[1]<0:\n",
    "                        break\n",
    "                \n",
    "                if balanceCounter[0]!=0 or balanceCounter[1]!=0:\n",
    "                    outputSeq = [0]*len(inputSeq)\n",
    "                else:\n",
    "                    outputSeq = [1]*len(inputSeq)\n",
    "\n",
    "                inputSeq.insert(0,\"BOS\")\n",
    "                outputSeq.insert(0,\"BOS\")\n",
    "\n",
    "                data[i] = (inputSeq, outputSeq)\n",
    "\n",
    "\n",
    "        case _:\n",
    "            print(name, \"is not an accepted name the accepted names are\",acceptedNamesAndInput)\n",
    "            return None\n",
    "\n",
    "    return data\n",
    "\n",
    "data = generateData(\"shuffle_dyck2\", 5, 100)\n",
    "print(data[:5])\n",
    "\n",
    "def generateModel(name: str, maxLength: int) -> Model:\n",
    "    model = None\n",
    "    match name:\n",
    "        case \"reverse\":\n",
    "            inputs = {t for t in acceptedNamesAndInput[name]}\n",
    "            model = Model(lib.make_reverse(rasp.tokens), inputs, maxLength, name)\n",
    "\n",
    "        case \"hist\":\n",
    "            inputs = {t for t in acceptedNamesAndInput[name]}\n",
    "            model = Model(lib.make_hist(), inputs, maxLength, name)\n",
    "\n",
    "        case \"sort\":\n",
    "            inputs = {t for t in acceptedNamesAndInput[name]}\n",
    "            model = Model(lib.make_sort(rasp.tokens, rasp.tokens, max_seq_len=maxLength, min_key=min(inputs)), inputs, maxLength, name)\n",
    "\n",
    "        case \"most-freq\":\n",
    "            inputs = {t for t in acceptedNamesAndInput[name]}\n",
    "            model = Model(lib.make_sort_freq(maxLength), inputs, maxLength, name)\n",
    "\n",
    "        case \"shuffle_dyck1\":\n",
    "            inputs = {t for t in acceptedNamesAndInput[name]}\n",
    "            model = Model(lib.make_shuffle_dyck([\"()\"]), inputs, maxLength, name)\n",
    "        \n",
    "        case \"shuffle_dyck2\":\n",
    "            inputs = {t for t in acceptedNamesAndInput[name]}\n",
    "            model = Model(lib.make_shuffle_dyck([\"()\",\"{}\"]), inputs, maxLength, name)\n",
    "\n",
    "        case _:\n",
    "            print(name, \"is not an accepted name the accepted names are\",acceptedNamesAndInput)\n",
    "            return None\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dyck1\n",
      "Percentage of data which is:\n",
      "Of odd length: 12.7\n",
      "Balanced: 42.62\n",
      "Percentage of data which is:\n",
      "Of odd length: 9.18\n",
      "Balanced: 48.76\n",
      "Percentage of data which is:\n",
      "Of odd length: 12.67\n",
      "Balanced: 49.12\n",
      "Percentage of data which is:\n",
      "Of odd length: 11.79\n",
      "Balanced: 51.0\n",
      "\n",
      " dyck2\n",
      "Percentage of data which is:\n",
      "Of odd length: 12.86\n",
      "Balanced: 41.09\n",
      "Percentage of data which is:\n",
      "Of odd length: 8.59\n",
      "Balanced: 47.35\n",
      "Percentage of data which is:\n",
      "Of odd length: 12.3\n",
      "Balanced: 47.25\n",
      "Percentage of data which is:\n",
      "Of odd length: 11.66\n",
      "Balanced: 51.44\n"
     ]
    }
   ],
   "source": [
    "#Prints some statistics on the generated dyck data\n",
    "def checkDyckBalance(data):\n",
    "    oddLength = 0\n",
    "    balanced = 0\n",
    "\n",
    "    for (input, output) in data:\n",
    "        if len(input)%2==0 :    #length + bos\n",
    "            oddLength +=1\n",
    "        if output[1]==1:\n",
    "            balanced+=1\n",
    "    \n",
    "    oddLength /= len(data)/100\n",
    "    balanced /= len(data)/100\n",
    "\n",
    "    print(\"Percentage of data which is:\")\n",
    "    print(\"Of odd length:\", oddLength)\n",
    "    print(\"Balanced:\", balanced)\n",
    "\n",
    "print(\"dyck1\")\n",
    "checkDyckBalance(generateData(\"shuffle_dyck1\", 5, 10000))\n",
    "checkDyckBalance(generateData(\"shuffle_dyck1\", 10, 10000))\n",
    "checkDyckBalance(generateData(\"shuffle_dyck1\", 15, 10000))\n",
    "checkDyckBalance(generateData(\"shuffle_dyck1\", 50, 10000))\n",
    "\n",
    "print(\"\\ndyck2\")\n",
    "checkDyckBalance(generateData(\"shuffle_dyck2\", 5, 10000))\n",
    "checkDyckBalance(generateData(\"shuffle_dyck2\", 10, 10000))\n",
    "checkDyckBalance(generateData(\"shuffle_dyck2\", 15, 10000))\n",
    "checkDyckBalance(generateData(\"shuffle_dyck2\", 50, 10000))\n",
    "\n",
    "#Seems to work fairly well, roughly between 40 and 50% is balanced depending on the maximum size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the boolean result for each case in the data set\n",
    "def evaluateModel(model: Model, data):\n",
    "    print(\"Evaluating model:\",model.name)\n",
    "    N=len(data)\n",
    "    booleanAccuracy = np.zeros(N)\n",
    "    \n",
    "    for i in range(N):\n",
    "        inputSeq, trueOutputSeq = data[i]\n",
    "        outputSeq = model.evaluate(inputSeq)\n",
    "\n",
    "        seqLength = len(trueOutputSeq)\n",
    "        sameToken = np.zeros(seqLength)\n",
    "        for ii in range(seqLength):\n",
    "            sameToken[ii] = (outputSeq[ii]==trueOutputSeq[ii])\n",
    "        \n",
    "        booleanAccuracy[i] = (np.sum(sameToken) == seqLength)\n",
    "\n",
    "        #Add loading bar to keep track of progress\n",
    "\n",
    "    return booleanAccuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['BOS', '}', '{'], ['BOS', 0, 0]), (['BOS', '{', '}'], ['BOS', 1, 1]), (['BOS', '}', '('], ['BOS', 0, 0]), (['BOS', '{', '}', ')', '{'], ['BOS', 0, 0, 0, 0]), (['BOS', '(', ')', '{', '}'], ['BOS', 1, 1, 1, 1])]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "name = \"shuffle_dyck2\"\n",
    "maxSeqLen = 5\n",
    "data = generateData(name, maxSeqLen, 1000)\n",
    "model = generateModel(name, maxSeqLen)\n",
    "\n",
    "print(data[:5])\n",
    "\n",
    "booleanAccuracy = evaluateModel(model, data)\n",
    "accuracy=np.mean(booleanAccuracy)\n",
    "print(\"Accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "(['BOS', '(', ')'], ['BOS', 1, 1])\n",
      "['BOS', True, True]\n"
     ]
    }
   ],
   "source": [
    "print(np.argwhere(booleanAccuracy-1))\n",
    "print(data[7])\n",
    "\n",
    "print(model.evaluate(data[7][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "cat = {0:1, 2:4, 1:6}\n",
    "print(len(cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerConfig(num_heads=1, num_layers=4, key_size=12, mlp_hidden_size=30, dropout_rate=0.0, activation_function=<jax._src.custom_derivatives.custom_jvp object at 0x000001C2B8206D80>, layer_norm=False, causal=False)\n",
      "\n",
      "Layer analysis:\n",
      "pos_embed <class 'dict'>\n",
      "\t embeddings <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "\t  N: 270\t min/max: 0.00/1.00\t nValues: 2\t percentageZero: 98.15\n",
      "token_embed <class 'dict'>\n",
      "\t embeddings <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "\t  N: 315\t min/max: 0.00/1.00\t nValues: 2\t percentageZero: 95.56\n",
      "transformer/layer_0/attn/key <class 'dict'>\n",
      "\t w <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "\t  N: 540\t min/max: 0.00/1.00\t nValues: 2\t percentageZero: 98.89\n",
      "transformer/layer_0/attn/linear <class 'dict'>\n",
      "\t w <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "\t  N: 540\t min/max: 0.00/1.00\t nValues: 2\t percentageZero: 99.81\n",
      "transformer/layer_0/attn/query <class 'dict'>\n",
      "\t w <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "\t  N: 540\t min/max: 0.00/100.00\t nValues: 2\t percentageZero: 95.19\n",
      "transformer/layer_0/attn/value <class 'dict'>\n",
      "\t w <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "\t  N: 540\t min/max: 0.00/1.00\t nValues: 2\t percentageZero: 99.81\n",
      "transformer/layer_0/mlp/linear_1 <class 'dict'>\n",
      "\t w <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "\t  N: 1350\t min/max: -75.00/100.00\t nValues: 13\t percentageZero: 98.37\n",
      "transformer/layer_0/mlp/linear_2 <class 'dict'>\n",
      "\t w <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "\t  N: 1350\t min/max: -1.00/1.00\t nValues: 3\t percentageZero: 98.30\n",
      "transformer/layer_1/attn/key <class 'dict'>\n",
      "\t w <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "\t  N: 540\t min/max: 0.00/0.00\t nValues: 1\t percentageZero: 100.00\n",
      "transformer/layer_1/attn/linear <class 'dict'>\n",
      "\t w <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "\t  N: 540\t min/max: 0.00/0.00\t nValues: 1\t percentageZero: 100.00\n",
      "transformer/layer_1/attn/query <class 'dict'>\n",
      "\t w <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "\t  N: 540\t min/max: 0.00/0.00\t nValues: 1\t percentageZero: 100.00\n",
      "transformer/layer_1/attn/value <class 'dict'>\n",
      "\t w <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "\t  N: 540\t min/max: 0.00/0.00\t nValues: 1\t percentageZero: 100.00\n",
      "transformer/layer_1/mlp/linear_1 <class 'dict'>\n",
      "\t w <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "\t  N: 1350\t min/max: -1.00/1.00\t nValues: 3\t percentageZero: 93.33\n",
      "transformer/layer_1/mlp/linear_2 <class 'dict'>\n",
      "\t w <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "\t  N: 1350\t min/max: 0.00/1.00\t nValues: 2\t percentageZero: 97.78\n",
      "transformer/layer_2/attn/key <class 'dict'>\n",
      "\t w <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "\t  N: 540\t min/max: 0.00/0.00\t nValues: 1\t percentageZero: 100.00\n",
      "transformer/layer_2/attn/linear <class 'dict'>\n",
      "\t w <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "\t  N: 540\t min/max: 0.00/0.00\t nValues: 1\t percentageZero: 100.00\n",
      "transformer/layer_2/attn/query <class 'dict'>\n",
      "\t w <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "\t  N: 540\t min/max: 0.00/0.00\t nValues: 1\t percentageZero: 100.00\n",
      "transformer/layer_2/attn/value <class 'dict'>\n",
      "\t w <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "\t  N: 540\t min/max: 0.00/0.00\t nValues: 1\t percentageZero: 100.00\n",
      "transformer/layer_2/mlp/linear_1 <class 'dict'>\n",
      "\t w <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "\t  N: 1350\t min/max: 0.00/1.00\t nValues: 2\t percentageZero: 99.26\n",
      "transformer/layer_2/mlp/linear_2 <class 'dict'>\n",
      "\t w <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "\t  N: 1350\t min/max: 0.00/1.00\t nValues: 2\t percentageZero: 99.26\n",
      "transformer/layer_3/attn/key <class 'dict'>\n",
      "\t w <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "\t  N: 540\t min/max: 0.00/1.00\t nValues: 2\t percentageZero: 98.89\n",
      "transformer/layer_3/attn/linear <class 'dict'>\n",
      "\t w <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "\t  N: 540\t min/max: 0.00/1.00\t nValues: 2\t percentageZero: 99.07\n",
      "transformer/layer_3/attn/query <class 'dict'>\n",
      "\t w <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "\t  N: 540\t min/max: 0.00/100.00\t nValues: 3\t percentageZero: 98.89\n",
      "transformer/layer_3/attn/value <class 'dict'>\n",
      "\t w <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "\t  N: 540\t min/max: 0.00/1.00\t nValues: 2\t percentageZero: 99.07\n",
      "transformer/layer_3/mlp/linear_1 <class 'dict'>\n",
      "\t w <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "\t  N: 1350\t min/max: 0.00/0.00\t nValues: 1\t percentageZero: 100.00\n",
      "transformer/layer_3/mlp/linear_2 <class 'dict'>\n",
      "\t w <class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "\t  N: 1350\t min/max: 0.00/0.00\t nValues: 1\t percentageZero: 100.00\n"
     ]
    }
   ],
   "source": [
    "def printWeightStatistics(weightCounter: dict):\n",
    "    totalValues = 0\n",
    "    for _, n in weightCounter.items():\n",
    "        totalValues+=n\n",
    "    maxValue = max(weightCounter)\n",
    "    minValue = min(weightCounter)\n",
    "    zeroPercentage = 100*weightCounter[0]/totalValues if 0 in weightCounter else 0\n",
    "    numberOfUniqueValues = len(weightCounter)\n",
    "    print(\"N: %d\\t min/max: %.2f/%.2f\\t nValues: %d\\t percentageZero: %.2f\" % \n",
    "          (totalValues, minValue, maxValue, numberOfUniqueValues, zeroPercentage))\n",
    "\n",
    "#Quick function to check for if all \"b\" weights\n",
    "def analyzeB(model: Model):\n",
    "    for name1, layer in model.model.params.items():\n",
    "        for name2, weight in layer.items():\n",
    "            if name2!=\"b\":\n",
    "                continue\n",
    "            weightCounter = {}\n",
    "\n",
    "            #Find unique weights and count instances for the weights\n",
    "            for t in weight.flatten():\n",
    "                t = float(t)\n",
    "                if t in weightCounter:\n",
    "                    weightCounter[t]+=1\n",
    "                else:\n",
    "                    weightCounter[t]=1\n",
    "\n",
    "            printWeightStatistics(weightCounter)\n",
    "\n",
    "            #print(\"\\t\", tt.min(), tt.max(), tt.mean())\n",
    "            # for ttt in model.model.params[t][tt]:\n",
    "            #     print(\"\\t\\t\", ttt, type(ttt))\n",
    "\n",
    "def analyzeWeights(model: Model):\n",
    "    print(model.model.model_config)\n",
    "    print(\"\\nLayer analysis:\")\n",
    "    totalCounter = {}\n",
    "    for name1, layer in model.model.params.items():\n",
    "        print(name1, type(layer))\n",
    "        for name2, weight in layer.items():\n",
    "            if name2==\"b\":  #Skips the b vectors since they are \n",
    "                continue\n",
    "            weightCounter = {}\n",
    "            print(\"\\t\", name2, type(weight))\n",
    "\n",
    "            #Find unique weights and count instances for the weights\n",
    "            for t in weight.flatten():\n",
    "                t = float(t)\n",
    "                if t in weightCounter:\n",
    "                    weightCounter[t]+=1\n",
    "                else:\n",
    "                    weightCounter[t]=1\n",
    "\n",
    "            print(\"\\t\",end=\"  \")\n",
    "            printWeightStatistics(weightCounter)\n",
    "\n",
    "#name = \"reverse\"\n",
    "name = \"reverse\"\n",
    "maxSeqLen = 5\n",
    "data = generateData(name, maxSeqLen, 1000)\n",
    "model = generateModel(name, maxSeqLen)\n",
    "\n",
    "#analyzeB(model)\n",
    "analyzeWeights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<class 'jaxlib.xla_extension.ArrayImpl'>' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[237], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer/layer_0/attn/key\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43maddNoise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[237], line 3\u001b[0m, in \u001b[0;36maddNoise\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maddNoise\u001b[39m(model: Model):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m#model.model.params[\"transformer/layer_0/attn/key\"][\"w\"] = model.model.params[\"transformer/layer_0/attn/key\"][\"w\"].at([0,0]).set(1)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransformer/layer_0/attn/key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer/layer_0/attn/key\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer/layer_0/attn/key\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\00tho\\anaconda3\\envs\\RASP\\Lib\\site-packages\\jax\\_src\\numpy\\array_methods.py:278\u001b[0m, in \u001b[0;36m_unimplemented_setitem\u001b[1;34m(self, i, x)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_unimplemented_setitem\u001b[39m(\u001b[38;5;28mself\u001b[39m, i, x):\n\u001b[0;32m    274\u001b[0m   msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object does not support item assignment. JAX arrays are \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    275\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimmutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor another .at[] method: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    277\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 278\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)))\n",
      "\u001b[1;31mTypeError\u001b[0m: '<class 'jaxlib.xla_extension.ArrayImpl'>' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html"
     ]
    }
   ],
   "source": [
    "def addNoise(model: Model):\n",
    "    #model.model.params[\"transformer/layer_0/attn/key\"][\"w\"] = model.model.params[\"transformer/layer_0/attn/key\"][\"w\"].at([0,0]).set(1)\n",
    "    weights = model.model.params[\"transformer/layer_0/attn/key\"][\"w\"]\n",
    "    weights = weights.\n",
    "    print(model.model.params[\"transformer/layer_0/attn/key\"][\"w\"][0,0])\n",
    "    print(model.model.params[\"transformer/layer_0/attn/key\"][\"w\"])\n",
    "    return\n",
    "\n",
    "addNoise(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: shuffle_dyck2\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "booleanAccuracy = evaluateModel(model, data)\n",
    "accuracy=np.mean(booleanAccuracy)\n",
    "print(\"Accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "#### Testing the base functions and generating the test data\n",
    "\n",
    "The sort function does not have a 100% accuracy. This only seems to apply when including input token 0, if only using 1 and up it seems to work. A cursory analysis would suggest that the min value in the sort function is multiplied with the indicies which makes it indistinguishable if the minimum value is 1.\n",
    "\n",
    "The most-freq function does not work in the same method as the original RASP paper (despite the Tracr paper claiming they recreated the RASP function in Tracr). Instead of backfilling with BOS tokens it simply sorts all tokens in groups. The most-freq function (make_sort_freq) is also hardcoded to only accept 1 as the min_key value for some reason. I could fix this but it is not really a high priority (and seemingly breaks the sort function)\n",
    "\n",
    "The most-freq function seems to fail sometimes (always?) when there are mutiple groups of the same count. Maybe they did not actually sort the output based on token grupings and only on frequency? Need to check. That apears to be the case. The Tracr make_sort_freq function is lazy and does not differentiate between tokens as long as the count is the same.\n",
    "\n",
    "Shuffle dyck \n",
    "* The RASP paper uses the tokens T, P and F to account for if a dyck-k sequence is legal, possible legal or not legal for each token in the sequence. The Tracr implementation on the other hand only uses 1 or 0 to show if the entire sequence is legal or not. This is a far simpler solution yet for some reason they explicitly claim that this is how it is implemented in the RASP paper in their code ???\n",
    "* If tokens are randomly selected most sequences will be unblanaced e.g. only even sequences can be balanced and if the sequence starts with a end token it wll be unblanced.\n",
    "* I should probably try to generate the sequence such that the probability of a balanced sequence is roughly 50%\n",
    "\n",
    "#### Analyzing weights\n",
    "\n",
    "What do I need to look out for? All of these should probably be applied layerwise (for each matrix of weights) and globally\n",
    "* Maximum/minimum values?\n",
    "* Binary values?\n",
    "* All same values?\n",
    "* Percentage which is 0?\n",
    "\n",
    "It seems like all of the \"b\" weights are zero vectors for the given models. As such I feel like I should mostly stay away from those vectors when adding noise and training\n",
    "\n",
    "Many of the layer weights are zero. The layer weights are usually binary or trinary, very rarely do the layer assume more values than 3.\n",
    "\n",
    "The percentage of values which are zero is usually between 90 and 100%.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RASP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

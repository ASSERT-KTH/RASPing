{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# The default of float16 can lead to discrepancies between outputs of\n",
    "# the compiled model and the RASP program.\n",
    "jax.config.update('jax_default_matmul_precision', 'float32')\n",
    "\n",
    "from tracr.compiler import compiling\n",
    "from tracr.compiler import lib\n",
    "from tracr.rasp import rasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates some weight statistic from a weight counter\n",
    "def calculateWeightStatistics(weightCounter: dict, doPrint = False):\n",
    "    totalValues = 0\n",
    "    for _, n in weightCounter.items():\n",
    "        totalValues+=n\n",
    "    maxValue = max(weightCounter)\n",
    "    minValue = min(weightCounter)\n",
    "    zeroPercentage = 100*weightCounter[0]/totalValues if 0 in weightCounter else 0\n",
    "    numberOfUniqueValues = len(weightCounter)\n",
    "\n",
    "    if doPrint:\n",
    "        print(\"N: %d\\t min/max: %.2f/%.2f\\t nValues: %d\\t percentageZero: %.2f\" % \n",
    "          (totalValues, minValue, maxValue, numberOfUniqueValues, zeroPercentage))\n",
    "    return {\"totalValues\":totalValues, \"maxValue\": maxValue, \"minValue\": minValue, \"zeroPercentage\": zeroPercentage, \"numberOfUniqueValues\": numberOfUniqueValues}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, raspFunction: rasp.SOp, inputs, seqLength: int, name: str):\n",
    "        self.raspFunction = raspFunction\n",
    "        self.inputs = inputs\n",
    "        self.seqLength = seqLength\n",
    "        self.model = compiling.compile_rasp_to_model(self.raspFunction, self.inputs, self.seqLength, compiler_bos=\"BOS\")\n",
    "        self.name = name\n",
    "\n",
    "        self.weightStatistics = {}\n",
    "        self.updateWeightStatistics()\n",
    "\n",
    "    def updateWeightStatistics(self):\n",
    "        self.weightStatistics = {}\n",
    "        \n",
    "        totalCounter = {}\n",
    "        for name1, layer in self.model.params.items():\n",
    "            self.weightStatistics[name1] = {}\n",
    "            #print(name1, type(layer))\n",
    "            for name2, weight in layer.items():\n",
    "                weightCounter = {}\n",
    "                #print(\"\\t\", name2, type(weight))\n",
    "\n",
    "                #Find unique weights and count instances for the weights\n",
    "                for t in weight.flatten():\n",
    "                    t = float(t)\n",
    "                    if t in weightCounter:\n",
    "                        weightCounter[t]+=1\n",
    "                    else:\n",
    "                        weightCounter[t]=1\n",
    "\n",
    "                #print(\"\\t\",end=\"  \")\n",
    "                self.weightStatistics[name1][name2] = calculateWeightStatistics(weightCounter)\n",
    "\n",
    "                #Appends the weight counts to the total counts\n",
    "                for number, count in weightCounter.items():\n",
    "                    if number in totalCounter:\n",
    "                        totalCounter[number]+=count\n",
    "                    else:\n",
    "                        totalCounter[number]=count\n",
    "\n",
    "        #print(\"\\nTotal statistics\")\n",
    "        self.weightStatistics[\"total\"] = calculateWeightStatistics(totalCounter)\n",
    "\n",
    "    def printWeightStatistics(self, includeB=False):\n",
    "        print(self.model.model_config)\n",
    "        print(\"\\nLayer analysis:\")\n",
    "\n",
    "        for name1, _ in self.weightStatistics.items():\n",
    "            print(name1)\n",
    "            if name1==\"total\":\n",
    "                weightStats=self.weightStatistics[name1]\n",
    "                print(\"\\t  N: %d\\t min/max: %.2f/%.2f\\t nValues: %d\\t percentageZero: %.2f\" % \n",
    "                    (weightStats[\"totalValues\"], weightStats[\"minValue\"], weightStats[\"maxValue\"], weightStats[\"numberOfUniqueValues\"], weightStats[\"zeroPercentage\"]))\n",
    "                continue\n",
    "            \n",
    "            for name2, weightStats in self.weightStatistics[name1].items():\n",
    "                if name2==\"b\" and includeB!=True:\n",
    "                    continue\n",
    "                print(\"\\t\", name2)\n",
    "                print(\"\\t  N: %d\\t min/max: %.2f/%.2f\\t nValues: %d\\t percentageZero: %.2f\" % \n",
    "                    (weightStats[\"totalValues\"], weightStats[\"minValue\"], weightStats[\"maxValue\"], weightStats[\"numberOfUniqueValues\"], weightStats[\"zeroPercentage\"]))\n",
    "    \n",
    "    \n",
    "    def evaluate(self, input):\n",
    "        return self.model.apply(input).decoded\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "inputs = [0,1,2,3,4,5]\n",
    "print(inputs[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['BOS', '{', '{', '}', '}'], ['BOS', 1, 1, 1, 1]), (['BOS', '{', '}', '{', ')'], ['BOS', 0, 0, 0, 0]), (['BOS', '(', ')', ')', '(', '('], ['BOS', 0, 0, 0, 0, 0]), (['BOS', '{', '}', '(', ')'], ['BOS', 1, 1, 1, 1]), (['BOS', '(', ')', '('], ['BOS', 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "acceptedNamesAndInput = {\"reverse\": [\"a\",\"b\",\"c\",\"d\",\"e\"], #Tokens doesn't matter much. Only the quantity influnce the results due to encoding (I think)\n",
    "                         \"hist\": [\"a\",\"b\",\"c\",\"d\"], #Tokens doesn't matter much. Only the quantity influnce the results due to encoding (I think)\n",
    "                         \"sort\": [1,2,3,4,5,6], #[0,1,2,3,4,5,6]    Seems to fail sometimes if 0 is included (irrespktive of if 0 is in the failed input or not, don't know why)\n",
    "                         \"most-freq\": [1,2,3,4,5],\n",
    "                         \"shuffle_dyck1\": [\"(\",\")\"],\n",
    "                         \"shuffle_dyck2\": [\"(\",\")\",\"{\",\"}\"]}     #Could theoretically be adapted into shuffle dyck-k but would still require unique tokens for each k\n",
    "\n",
    "def generateData(name: str, maxSeqLength: int, size: int):\n",
    "    data = [None]*size\n",
    "\n",
    "    match name:\n",
    "        case \"reverse\":\n",
    "            acceptedTokens = acceptedNamesAndInput[name]\n",
    "\n",
    "            for i in range(size):\n",
    "                inputLength = np.random.randint(2, maxSeqLength+1)  #Uniformly distributed between 2 and max length\n",
    "\n",
    "                inputSeq = []\n",
    "                outputSeq = []\n",
    "                for t in np.random.choice(acceptedTokens, inputLength):\n",
    "                    inputSeq.append(t)\n",
    "                    outputSeq.insert(0,t)\n",
    "                inputSeq.insert(0,\"BOS\")\n",
    "                outputSeq.insert(0,\"BOS\")\n",
    "\n",
    "                data[i] = (inputSeq, outputSeq)\n",
    "\n",
    "        case \"hist\":\n",
    "            acceptedTokens = acceptedNamesAndInput[name]  \n",
    "            \n",
    "            for i in range(size):\n",
    "                inputLength = np.random.randint(2, maxSeqLength+1)  #Uniformly distributed between 2 and max length\n",
    "\n",
    "                inputSeq = []\n",
    "                tokenCounter = dict(zip(acceptedTokens, [0]*len(acceptedTokens)))   #Counter built during generating input\n",
    "                for t in np.random.choice(acceptedTokens, inputLength):\n",
    "                    inputSeq.append(t)\n",
    "                    tokenCounter[t]+=1\n",
    "    \n",
    "                outputSeq = []\n",
    "                for t in inputSeq:  #Fill output according to token counter\n",
    "                    outputSeq.append(tokenCounter[t])\n",
    "\n",
    "                inputSeq.insert(0,\"BOS\")\n",
    "                outputSeq.insert(0,\"BOS\")\n",
    "\n",
    "                data[i] = (inputSeq, outputSeq)\n",
    "\n",
    "        case \"sort\":\n",
    "            acceptedTokens = acceptedNamesAndInput[name]  \n",
    "            \n",
    "            for i in range(size):\n",
    "                inputLength = np.random.randint(2, maxSeqLength+1)  #Uniformly distributed between 2 and max length\n",
    "\n",
    "                inputSeq = []\n",
    "                outputSeq = []\n",
    "                for t in np.random.choice(acceptedTokens, inputLength):\n",
    "                    inputSeq.append(t)\n",
    "                    outputSeq.append(t)\n",
    "    \n",
    "                inputSeq.insert(0,\"BOS\")\n",
    "                outputSeq.sort()\n",
    "                outputSeq.insert(0,\"BOS\")\n",
    "\n",
    "                data[i] = (inputSeq, outputSeq)\n",
    "\n",
    "        case \"most-freq\":   #sort based on most frequent token with original position as tie breaker\n",
    "            acceptedTokens = acceptedNamesAndInput[name]  \n",
    "\n",
    "            for i in range(size):\n",
    "                inputLength = np.random.randint(2, maxSeqLength+1)  #Uniformly distributed between 2 and max length\n",
    "\n",
    "                inputSeq = []\n",
    "                tempSeq = []\n",
    "                tokenCounter = dict(zip(acceptedTokens, [0]*len(acceptedTokens)))   #Counter built during generating input\n",
    "                for t in np.random.choice(acceptedTokens, inputLength):\n",
    "                    inputSeq.append(t)\n",
    "                    tokenCounter[t]+=1\n",
    "                    tempSeq.append(t)    \n",
    "                \n",
    "                tempSeq.sort(key = (lambda x: -tokenCounter[x]))  #Sort the list in descending order of frequency\n",
    "\n",
    "                outputSeq = tempSeq\n",
    "\n",
    "                #Groups the tokens (Apparently not done by the Tracr solution)\n",
    "                \"\"\"\n",
    "                outputSeq = []\n",
    "                for t in tempSeq:\n",
    "                    if t not in outputSeq:\n",
    "                        for ii in range(tokenCounter[t]):\n",
    "                            outputSeq.append(t)\n",
    "                \"\"\"\n",
    "\n",
    "                inputSeq.insert(0,\"BOS\")\n",
    "                outputSeq.insert(0,\"BOS\")\n",
    "\n",
    "                data[i] = (inputSeq, outputSeq)\n",
    "\n",
    "        case \"shuffle_dyck1\":\n",
    "            acceptedTokens = acceptedNamesAndInput[name]\n",
    "\n",
    "            for i in range(size):\n",
    "                for ii in range(3):     #Ensures that roughly one out of eight sequences has an odd length\n",
    "                    inputLength = np.random.randint(2, maxSeqLength+1)  #Uniformly distributed between 2 and max length\n",
    "                    if inputLength%2==0:\n",
    "                        break\n",
    "\n",
    "                inputSeq = []\n",
    "                tokenCount = {\"(\":0,\")\":0}\n",
    "                tokenProb = np.zeros(len(acceptedTokens))   #Live probabilty distribution to more evenly distribute the balanced and unblanaced sequences\n",
    "                tokenProb[1] = 1/(inputLength+1)\n",
    "                tokenProb[0] = 1 - tokenProb[1]\n",
    "\n",
    "                #Build the sequence token by token and ensuring the probability of drawing a balanced sequence is always higher than drawing an unbalanced sequence\n",
    "                for ind in range(inputLength):\n",
    "                    t = np.random.choice(acceptedTokens, 1, p=tokenProb)[0]\n",
    "                    tokenCount[t]+=1\n",
    "                    inputSeq.append(t)\n",
    "\n",
    "                    tokenDiff = tokenCount[\"(\"]-tokenCount[\")\"]\n",
    "                    if tokenDiff == 0:  #High probability of begining paranthesis if balanced\n",
    "                        tokenProb[1] = 1/(inputLength+1)\n",
    "                        tokenProb[0] = 1 - tokenProb[1]\n",
    "                    elif tokenDiff > 0:   #High probability of end paranthesis if more begining paranthesis\n",
    "                        tokenProb[0] = 1/((inputLength+1)*tokenDiff)\n",
    "                        tokenProb[1] = 1 - tokenProb[0]\n",
    "                    else: #High probability of begining paranthesis if more end paranthesis\n",
    "                        tokenProb[1] = 1/((inputLength+1)*(-tokenDiff))\n",
    "                        tokenProb[0] = 1 - tokenProb[1]\n",
    "                \n",
    "                #Checks for balance\n",
    "                balanceCounter=0\n",
    "                for t in inputSeq:\n",
    "                    if t==\"(\":\n",
    "                        balanceCounter+=1\n",
    "                    else:\n",
    "                        balanceCounter-=1\n",
    "                    if balanceCounter<0:\n",
    "                        break\n",
    "                \n",
    "                if balanceCounter!=0:\n",
    "                    outputSeq = [0]*len(inputSeq)\n",
    "                else:\n",
    "                    outputSeq = [1]*len(inputSeq)\n",
    "\n",
    "                inputSeq.insert(0,\"BOS\")\n",
    "                outputSeq.insert(0,\"BOS\")\n",
    "\n",
    "                data[i] = (inputSeq, outputSeq)\n",
    "\n",
    "        case \"shuffle_dyck2\":\n",
    "            acceptedTokens = acceptedNamesAndInput[name]\n",
    "\n",
    "            for i in range(size):\n",
    "                for ii in range(3):     #Ensures that roughly one out of eight sequences has an odd length\n",
    "                    inputLength = np.random.randint(2, maxSeqLength+1)  #Uniformly distributed between 2 and max length\n",
    "                    if inputLength%2==0:\n",
    "                        break\n",
    "\n",
    "                inputSeq = []\n",
    "                tokenCount = {\"(\":0,\")\":0,\"{\":0,\"}\":0}\n",
    "                tokenProb = np.zeros(len(acceptedTokens))   #Live probabilty distribution to more evenly distribute the balanced and unblanaced sequences\n",
    "                tokenProb[1] = 1/((inputLength+1)*2)\n",
    "                tokenProb[0] = 1/2 - tokenProb[1]\n",
    "                tokenProb[3] = tokenProb[1]\n",
    "                tokenProb[2] = tokenProb[0]\n",
    "\n",
    "                #Build the sequence token by token and ensuring the probability of drawing a balanced sequence is always higher than drawing an unbalanced sequence\n",
    "                for ind in range(inputLength):\n",
    "                    t = np.random.choice(acceptedTokens, 1, p=tokenProb)[0]\n",
    "                    tokenCount[t]+=1\n",
    "                    inputSeq.append(t)\n",
    "\n",
    "                    tokenDiff1 = tokenCount[\"(\"]-tokenCount[\")\"]\n",
    "                    tokenDiff2 = tokenCount[\"{\"]-tokenCount[\"}\"]\n",
    "                    if tokenDiff1 == 0 and tokenDiff2==0:  #High probability of begining paranthesis if balanced\n",
    "                        tokenProb[1] = 1/((inputLength+1)*2)\n",
    "                        tokenProb[0] = 1/2 - tokenProb[1]\n",
    "                        tokenProb[3] = tokenProb[1]\n",
    "                        tokenProb[2] = tokenProb[0]\n",
    "                    #High probability of end paranthesis if more begining paranthesis\n",
    "                    elif tokenDiff2 > 0 and tokenDiff1 > 0:\n",
    "                        tokenProb[0] = 1/((inputLength+1)*tokenDiff1*2)\n",
    "                        tokenProb[2] = 1/((inputLength+1)*tokenDiff2*2)\n",
    "                        tokenProb[1] = 1/2 - tokenProb[0]\n",
    "                        tokenProb[3] = 1/2 - tokenProb[2]\n",
    "                    elif tokenDiff1 > 0 and tokenDiff2==0: \n",
    "                        tokenProb[1] = 1 - 1/((inputLength+1)*tokenDiff1)\n",
    "                        split = 1 - tokenProb[1]    #The reminder of probability to distribute\n",
    "                        tokenProb[2] = split - split/((inputLength+1))    #More likely to start a new parenthesis than break sequence\n",
    "                        split = split - tokenProb[2]\n",
    "                        tokenProb[0] = split/2\n",
    "                        tokenProb[3] = split/2\n",
    "                    elif tokenDiff2 > 0 and tokenDiff1==0:   \n",
    "                        tokenProb[3] = 1 - 1/((inputLength+1)*tokenDiff2)\n",
    "                        split = 1 - tokenProb[3]    #The reminder of probability to distribute\n",
    "                        tokenProb[0] = split - split/((inputLength+1))    #More likely to start a new parenthesis than break sequence\n",
    "                        split = split - tokenProb[0]\n",
    "                        tokenProb[1] = split/2\n",
    "                        tokenProb[2] = split/2\n",
    "                    #High probability of begining paranthesis if more end paranthesis\n",
    "                    elif tokenDiff2 < 0 and tokenDiff1 < 0:\n",
    "                        tokenProb[1] = 1/((inputLength+1)*(-tokenDiff1)*2)\n",
    "                        tokenProb[3] = 1/((inputLength+1)*(-tokenDiff2)*2)\n",
    "                        tokenProb[0] = 1/2 - tokenProb[1]\n",
    "                        tokenProb[2] = 1/2 - tokenProb[3]\n",
    "                    elif tokenDiff1 < 0 and tokenDiff2 == 0:\n",
    "                        tokenProb[0] = 1 - 1/((inputLength+1)*(-tokenDiff1))\n",
    "                        split = 1 - tokenProb[0]    #The reminder of probability to distribute\n",
    "                        tokenProb[2] = split - split/((inputLength+1))    #More likely to start a new parenthesis than break sequence\n",
    "                        split = split - tokenProb[2]\n",
    "                        tokenProb[1] = split/2\n",
    "                        tokenProb[3] = split/2\n",
    "                    elif tokenDiff2 < 0 and tokenDiff1 == 0:\n",
    "                        tokenProb[2] = 1 - 1/((inputLength+1)*(-tokenDiff2))\n",
    "                        split = 1 - tokenProb[2]    #The reminder of probability to distribute\n",
    "                        tokenProb[0] = split - split/((inputLength+1))    #More likely to start a new parenthesis than break sequence\n",
    "                        split = split - tokenProb[0]\n",
    "                        tokenProb[1] = split/2\n",
    "                        tokenProb[3] = split/2\n",
    "                    #Higher probability to balance the sequence if currently unbalanced\n",
    "                    elif tokenDiff1 > 0 and tokenDiff2 < 0:\n",
    "                        tokenProb[1] = 1/((inputLength+1)*tokenDiff1*2)\n",
    "                        tokenProb[2] = 1/((inputLength+1)*(-tokenDiff2)*2)\n",
    "                        tokenProb[0] = 1/2 - tokenProb[1]\n",
    "                        tokenProb[3] = 1/2 - tokenProb[2]\n",
    "                    elif tokenDiff2 > 0 and tokenDiff1 < 0:\n",
    "                        tokenProb[3] = 1/((inputLength+1)*tokenDiff2*2)\n",
    "                        tokenProb[0] = 1/((inputLength+1)*(-tokenDiff1)*2)\n",
    "                        tokenProb[1] = 1/2 - tokenProb[0]\n",
    "                        tokenProb[2] = 1/2 - tokenProb[3]\n",
    "                \n",
    "                #Checks for balance\n",
    "                balanceCounter=[0,0]\n",
    "                for t in inputSeq:\n",
    "                    if t==\"(\":\n",
    "                        balanceCounter[0]+=1\n",
    "                    if t==\")\":\n",
    "                        balanceCounter[0]-=1\n",
    "                    if t==\"{\":\n",
    "                        balanceCounter[1]+=1\n",
    "                    if t==\"}\":\n",
    "                        balanceCounter[1]-=1\n",
    "                    \n",
    "                    if balanceCounter[0]<0 or balanceCounter[1]<0:\n",
    "                        break\n",
    "                \n",
    "                if balanceCounter[0]!=0 or balanceCounter[1]!=0:\n",
    "                    outputSeq = [0]*len(inputSeq)\n",
    "                else:\n",
    "                    outputSeq = [1]*len(inputSeq)\n",
    "\n",
    "                inputSeq.insert(0,\"BOS\")\n",
    "                outputSeq.insert(0,\"BOS\")\n",
    "\n",
    "                data[i] = (inputSeq, outputSeq)\n",
    "\n",
    "\n",
    "        case _:\n",
    "            print(name, \"is not an accepted name the accepted names are\",acceptedNamesAndInput)\n",
    "            return None\n",
    "\n",
    "    return data\n",
    "\n",
    "data = generateData(\"shuffle_dyck2\", 5, 100)\n",
    "print(data[:5])\n",
    "\n",
    "def generateModel(name: str, maxLength: int) -> Model:\n",
    "    model = None\n",
    "    match name:\n",
    "        case \"reverse\":\n",
    "            inputs = {t for t in acceptedNamesAndInput[name]}\n",
    "            model = Model(lib.make_reverse(rasp.tokens), inputs, maxLength, name)\n",
    "\n",
    "        case \"hist\":\n",
    "            inputs = {t for t in acceptedNamesAndInput[name]}\n",
    "            model = Model(lib.make_hist(), inputs, maxLength, name)\n",
    "\n",
    "        case \"sort\":\n",
    "            inputs = {t for t in acceptedNamesAndInput[name]}\n",
    "            model = Model(lib.make_sort(rasp.tokens, rasp.tokens, max_seq_len=maxLength, min_key=min(inputs)), inputs, maxLength, name)\n",
    "\n",
    "        case \"most-freq\":\n",
    "            inputs = {t for t in acceptedNamesAndInput[name]}\n",
    "            model = Model(lib.make_sort_freq(maxLength), inputs, maxLength, name)\n",
    "\n",
    "        case \"shuffle_dyck1\":\n",
    "            inputs = {t for t in acceptedNamesAndInput[name]}\n",
    "            model = Model(lib.make_shuffle_dyck([\"()\"]), inputs, maxLength, name)\n",
    "        \n",
    "        case \"shuffle_dyck2\":\n",
    "            inputs = {t for t in acceptedNamesAndInput[name]}\n",
    "            model = Model(lib.make_shuffle_dyck([\"()\",\"{}\"]), inputs, maxLength, name)\n",
    "\n",
    "        case _:\n",
    "            print(name, \"is not an accepted name the accepted names are\",acceptedNamesAndInput)\n",
    "            return None\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dyck1\n",
      "Percentage of data which is:\n",
      "Of odd length: 12.13\n",
      "Balanced: 43.09\n",
      "Percentage of data which is:\n",
      "Of odd length: 9.33\n",
      "Balanced: 49.61\n",
      "Percentage of data which is:\n",
      "Of odd length: 12.62\n",
      "Balanced: 47.66\n",
      "Percentage of data which is:\n",
      "Of odd length: 11.73\n",
      "Balanced: 52.82\n",
      "\n",
      "dyck2\n",
      "Percentage of data which is:\n",
      "Of odd length: 12.46\n",
      "Balanced: 40.47\n",
      "Percentage of data which is:\n",
      "Of odd length: 8.95\n",
      "Balanced: 47.3\n",
      "Percentage of data which is:\n",
      "Of odd length: 12.17\n",
      "Balanced: 46.7\n",
      "Percentage of data which is:\n",
      "Of odd length: 11.57\n",
      "Balanced: 50.21\n"
     ]
    }
   ],
   "source": [
    "#Prints some statistics on the generated dyck data\n",
    "def checkDyckBalance(data):\n",
    "    oddLength = 0\n",
    "    balanced = 0\n",
    "\n",
    "    for (input, output) in data:\n",
    "        if len(input)%2==0 :    #length + bos\n",
    "            oddLength +=1\n",
    "        if output[1]==1:\n",
    "            balanced+=1\n",
    "    \n",
    "    oddLength /= len(data)/100\n",
    "    balanced /= len(data)/100\n",
    "\n",
    "    print(\"Percentage of data which is:\")\n",
    "    print(\"Of odd length:\", oddLength)\n",
    "    print(\"Balanced:\", balanced)\n",
    "\n",
    "print(\"dyck1\")\n",
    "checkDyckBalance(generateData(\"shuffle_dyck1\", 5, 10000))\n",
    "checkDyckBalance(generateData(\"shuffle_dyck1\", 10, 10000))\n",
    "checkDyckBalance(generateData(\"shuffle_dyck1\", 15, 10000))\n",
    "checkDyckBalance(generateData(\"shuffle_dyck1\", 50, 10000))\n",
    "\n",
    "print(\"\\ndyck2\")\n",
    "checkDyckBalance(generateData(\"shuffle_dyck2\", 5, 10000))\n",
    "checkDyckBalance(generateData(\"shuffle_dyck2\", 10, 10000))\n",
    "checkDyckBalance(generateData(\"shuffle_dyck2\", 15, 10000))\n",
    "checkDyckBalance(generateData(\"shuffle_dyck2\", 50, 10000))\n",
    "\n",
    "#Seems to work fairly well, roughly between 40 and 50% is balanced depending on the maximum size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the boolean result for each case in the data set\n",
    "def evaluateModel(model: Model, data):\n",
    "    print(\"Evaluating model:\",model.name)\n",
    "    N=len(data)\n",
    "    booleanAccuracy = np.zeros(N)\n",
    "    \n",
    "    for i in range(N):\n",
    "        inputSeq, trueOutputSeq = data[i]\n",
    "        outputSeq = model.evaluate(inputSeq)\n",
    "\n",
    "        seqLength = len(trueOutputSeq)\n",
    "        sameToken = np.zeros(seqLength)\n",
    "        for ii in range(seqLength):\n",
    "            sameToken[ii] = (outputSeq[ii]==trueOutputSeq[ii])\n",
    "        \n",
    "        booleanAccuracy[i] = (np.sum(sameToken) == seqLength)\n",
    "\n",
    "        #Add loading bar to keep track of progress\n",
    "\n",
    "    return booleanAccuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['BOS', '(', ')', '(', ')'], ['BOS', 1, 1, 1, 1]), (['BOS', '{', '}', '{', '('], ['BOS', 0, 0, 0, 0]), (['BOS', '{', '}', '(', ')'], ['BOS', 1, 1, 1, 1]), (['BOS', '(', ')', '(', ')'], ['BOS', 1, 1, 1, 1]), (['BOS', '(', ')', '{'], ['BOS', 0, 0, 0])]\n",
      "Evaluating model: shuffle_dyck2\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "name = \"shuffle_dyck2\"\n",
    "maxSeqLen = 5\n",
    "data = generateData(name, maxSeqLen, 1000)\n",
    "model = generateModel(name, maxSeqLen)\n",
    "\n",
    "print(data[:5])\n",
    "\n",
    "booleanAccuracy = evaluateModel(model, data)\n",
    "accuracy=np.mean(booleanAccuracy)\n",
    "print(\"Accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "(['BOS', '{', '('], ['BOS', 0, 0])\n",
      "['BOS', False, False]\n"
     ]
    }
   ],
   "source": [
    "print(np.argwhere(booleanAccuracy-1))\n",
    "print(data[7])\n",
    "\n",
    "print(model.evaluate(data[7][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "cat = {0:1, 2:4, 1:6}\n",
    "print(len(cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerConfig(num_heads=1, num_layers=4, key_size=12, mlp_hidden_size=30, dropout_rate=0.0, activation_function=<jax._src.custom_derivatives.custom_jvp object at 0x000002B8A6099820>, layer_norm=False, causal=False)\n",
      "\n",
      "Layer analysis:\n",
      "pos_embed\n",
      "\t embeddings\n",
      "\t  N: 270\t min/max: 0.00/1.00\t nValues: 2\t percentageZero: 98.15\n",
      "token_embed\n",
      "\t embeddings\n",
      "\t  N: 315\t min/max: 0.00/1.00\t nValues: 2\t percentageZero: 95.56\n",
      "transformer/layer_0/attn/key\n",
      "\t w\n",
      "\t  N: 540\t min/max: 0.00/1.00\t nValues: 2\t percentageZero: 98.89\n",
      "transformer/layer_0/attn/linear\n",
      "\t w\n",
      "\t  N: 540\t min/max: 0.00/1.00\t nValues: 2\t percentageZero: 99.81\n",
      "transformer/layer_0/attn/query\n",
      "\t w\n",
      "\t  N: 540\t min/max: 0.00/100.00\t nValues: 2\t percentageZero: 95.19\n",
      "transformer/layer_0/attn/value\n",
      "\t w\n",
      "\t  N: 540\t min/max: 0.00/1.00\t nValues: 2\t percentageZero: 99.81\n",
      "transformer/layer_0/mlp/linear_1\n",
      "\t w\n",
      "\t  N: 1350\t min/max: -75.00/100.00\t nValues: 13\t percentageZero: 98.37\n",
      "transformer/layer_0/mlp/linear_2\n",
      "\t w\n",
      "\t  N: 1350\t min/max: -1.00/1.00\t nValues: 3\t percentageZero: 98.30\n",
      "transformer/layer_1/attn/key\n",
      "\t w\n",
      "\t  N: 540\t min/max: 0.00/0.00\t nValues: 1\t percentageZero: 100.00\n",
      "transformer/layer_1/attn/linear\n",
      "\t w\n",
      "\t  N: 540\t min/max: 0.00/0.00\t nValues: 1\t percentageZero: 100.00\n",
      "transformer/layer_1/attn/query\n",
      "\t w\n",
      "\t  N: 540\t min/max: 0.00/0.00\t nValues: 1\t percentageZero: 100.00\n",
      "transformer/layer_1/attn/value\n",
      "\t w\n",
      "\t  N: 540\t min/max: 0.00/0.00\t nValues: 1\t percentageZero: 100.00\n",
      "transformer/layer_1/mlp/linear_1\n",
      "\t w\n",
      "\t  N: 1350\t min/max: -1.00/1.00\t nValues: 3\t percentageZero: 93.33\n",
      "transformer/layer_1/mlp/linear_2\n",
      "\t w\n",
      "\t  N: 1350\t min/max: 0.00/1.00\t nValues: 2\t percentageZero: 97.78\n",
      "transformer/layer_2/attn/key\n",
      "\t w\n",
      "\t  N: 540\t min/max: 0.00/0.00\t nValues: 1\t percentageZero: 100.00\n",
      "transformer/layer_2/attn/linear\n",
      "\t w\n",
      "\t  N: 540\t min/max: 0.00/0.00\t nValues: 1\t percentageZero: 100.00\n",
      "transformer/layer_2/attn/query\n",
      "\t w\n",
      "\t  N: 540\t min/max: 0.00/0.00\t nValues: 1\t percentageZero: 100.00\n",
      "transformer/layer_2/attn/value\n",
      "\t w\n",
      "\t  N: 540\t min/max: 0.00/0.00\t nValues: 1\t percentageZero: 100.00\n",
      "transformer/layer_2/mlp/linear_1\n",
      "\t w\n",
      "\t  N: 1350\t min/max: 0.00/1.00\t nValues: 2\t percentageZero: 99.26\n",
      "transformer/layer_2/mlp/linear_2\n",
      "\t w\n",
      "\t  N: 1350\t min/max: 0.00/1.00\t nValues: 2\t percentageZero: 99.26\n",
      "transformer/layer_3/attn/key\n",
      "\t w\n",
      "\t  N: 540\t min/max: 0.00/1.00\t nValues: 2\t percentageZero: 98.89\n",
      "transformer/layer_3/attn/linear\n",
      "\t w\n",
      "\t  N: 540\t min/max: 0.00/1.00\t nValues: 2\t percentageZero: 99.07\n",
      "transformer/layer_3/attn/query\n",
      "\t w\n",
      "\t  N: 540\t min/max: 0.00/100.00\t nValues: 3\t percentageZero: 98.89\n",
      "transformer/layer_3/attn/value\n",
      "\t w\n",
      "\t  N: 540\t min/max: 0.00/1.00\t nValues: 2\t percentageZero: 99.07\n",
      "transformer/layer_3/mlp/linear_1\n",
      "\t w\n",
      "\t  N: 1350\t min/max: 0.00/0.00\t nValues: 1\t percentageZero: 100.00\n",
      "transformer/layer_3/mlp/linear_2\n",
      "\t w\n",
      "\t  N: 1350\t min/max: 0.00/0.00\t nValues: 1\t percentageZero: 100.00\n",
      "total\n",
      "\t  N: 20649\t min/max: -75.00/100.00\t nValues: 15\t percentageZero: 98.74\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Quick function to check for if all \"b\" weights are truly zero\n",
    "def analyzeB(model: Model):\n",
    "    for name1, layer in model.model.params.items():\n",
    "        for name2, weight in layer.items():\n",
    "            if name2!=\"b\":\n",
    "                continue\n",
    "            weightCounter = {}\n",
    "\n",
    "            #Find unique weights and count instances for the weights\n",
    "            for t in weight.flatten():\n",
    "                t = float(t)\n",
    "                if t in weightCounter:\n",
    "                    weightCounter[t]+=1\n",
    "                else:\n",
    "                    weightCounter[t]=1\n",
    "\n",
    "            calculateWeightStatistics(weightCounter, True)\n",
    "\n",
    "#name = \"reverse\"\n",
    "name = \"reverse\"\n",
    "maxSeqLen = 5\n",
    "data = generateData(name, maxSeqLen, 1000)\n",
    "model = generateModel(name, maxSeqLen)\n",
    "\n",
    "#analyzeB(model)\n",
    "model.updateWeightStatistics()\n",
    "model.printWeightStatistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerConfig(num_heads=1, num_layers=4, key_size=12, mlp_hidden_size=30, dropout_rate=0.0, activation_function=<jax._src.custom_derivatives.custom_jvp object at 0x000002B8A6099820>, layer_norm=False, causal=False)\n",
      "\n",
      "Layer analysis:\n",
      "pos_embed\n",
      "\t embeddings\n",
      "\t  N: 270\t min/max: 0.00/1.00\t nValues: 2\t percentageZero: 98.15\n",
      "token_embed\n",
      "\t embeddings\n",
      "\t  N: 315\t min/max: 0.00/1.00\t nValues: 2\t percentageZero: 95.56\n",
      "transformer/layer_0/attn/key\n",
      "\t w\n",
      "\t  N: 540\t min/max: -0.00/1.00\t nValues: 540\t percentageZero: 0.00\n",
      "transformer/layer_0/attn/linear\n",
      "\t w\n",
      "\t  N: 540\t min/max: -0.00/1.00\t nValues: 540\t percentageZero: 0.00\n",
      "transformer/layer_0/attn/query\n",
      "\t w\n",
      "\t  N: 540\t min/max: -0.00/100.00\t nValues: 539\t percentageZero: 0.00\n",
      "transformer/layer_0/attn/value\n",
      "\t w\n",
      "\t  N: 540\t min/max: -0.00/1.00\t nValues: 540\t percentageZero: 0.00\n",
      "transformer/layer_0/mlp/linear_1\n",
      "\t w\n",
      "\t  N: 1350\t min/max: -75.00/100.00\t nValues: 1350\t percentageZero: 0.00\n",
      "transformer/layer_0/mlp/linear_2\n",
      "\t w\n",
      "\t  N: 1350\t min/max: -1.00/1.00\t nValues: 1350\t percentageZero: 0.00\n",
      "transformer/layer_1/attn/key\n",
      "\t w\n",
      "\t  N: 540\t min/max: -0.00/0.00\t nValues: 540\t percentageZero: 0.00\n",
      "transformer/layer_1/attn/linear\n",
      "\t w\n",
      "\t  N: 540\t min/max: -0.00/0.00\t nValues: 540\t percentageZero: 0.00\n",
      "transformer/layer_1/attn/query\n",
      "\t w\n",
      "\t  N: 540\t min/max: -0.00/0.00\t nValues: 540\t percentageZero: 0.00\n",
      "transformer/layer_1/attn/value\n",
      "\t w\n",
      "\t  N: 540\t min/max: -0.00/0.00\t nValues: 540\t percentageZero: 0.00\n",
      "transformer/layer_1/mlp/linear_1\n",
      "\t w\n",
      "\t  N: 1350\t min/max: -1.00/1.00\t nValues: 1350\t percentageZero: 0.00\n",
      "transformer/layer_1/mlp/linear_2\n",
      "\t w\n",
      "\t  N: 1350\t min/max: -0.00/1.00\t nValues: 1350\t percentageZero: 0.00\n",
      "transformer/layer_2/attn/key\n",
      "\t w\n",
      "\t  N: 540\t min/max: -0.00/0.00\t nValues: 540\t percentageZero: 0.00\n",
      "transformer/layer_2/attn/linear\n",
      "\t w\n",
      "\t  N: 540\t min/max: -0.00/0.00\t nValues: 540\t percentageZero: 0.00\n",
      "transformer/layer_2/attn/query\n",
      "\t w\n",
      "\t  N: 540\t min/max: -0.00/0.00\t nValues: 540\t percentageZero: 0.00\n",
      "transformer/layer_2/attn/value\n",
      "\t w\n",
      "\t  N: 540\t min/max: -0.00/0.00\t nValues: 540\t percentageZero: 0.00\n",
      "transformer/layer_2/mlp/linear_1\n",
      "\t w\n",
      "\t  N: 1350\t min/max: -0.00/1.00\t nValues: 1350\t percentageZero: 0.00\n",
      "transformer/layer_2/mlp/linear_2\n",
      "\t w\n",
      "\t  N: 1350\t min/max: -0.00/1.00\t nValues: 1350\t percentageZero: 0.00\n",
      "transformer/layer_3/attn/key\n",
      "\t w\n",
      "\t  N: 540\t min/max: -0.00/1.00\t nValues: 540\t percentageZero: 0.00\n",
      "transformer/layer_3/attn/linear\n",
      "\t w\n",
      "\t  N: 540\t min/max: -0.00/1.00\t nValues: 540\t percentageZero: 0.00\n",
      "transformer/layer_3/attn/query\n",
      "\t w\n",
      "\t  N: 540\t min/max: -0.00/100.00\t nValues: 540\t percentageZero: 0.00\n",
      "transformer/layer_3/attn/value\n",
      "\t w\n",
      "\t  N: 540\t min/max: -0.00/1.00\t nValues: 540\t percentageZero: 0.00\n",
      "transformer/layer_3/mlp/linear_1\n",
      "\t w\n",
      "\t  N: 1350\t min/max: -0.00/0.00\t nValues: 1350\t percentageZero: 0.00\n",
      "transformer/layer_3/mlp/linear_2\n",
      "\t w\n",
      "\t  N: 1350\t min/max: -0.00/0.00\t nValues: 1350\t percentageZero: 0.00\n",
      "total\n",
      "\t  N: 20649\t min/max: -75.00/100.00\t nValues: 19437\t percentageZero: 5.76\n"
     ]
    }
   ],
   "source": [
    "#Add noise to the model weights according too noiseType, amount and param\n",
    "def addNoise(model: Model, noiseType = \"bitFlip\", amount=1, param = 0.1, includeEncoding = False):\n",
    "    noiseTypes = [\"bitFlip\", \"gaussian\", \"flipFirst\", \"temp\"]\n",
    "    if noiseType not in noiseTypes:\n",
    "        print(\"Error: noiseType needs to be one of\", noiseTypes)\n",
    "        return\n",
    "    \n",
    "    match noiseType:\n",
    "        #Flip binary bits \n",
    "        #If amount is a integer it flips that many random bits, if it is float it flips that fraction of bits\n",
    "        case \"bitFlip\":\n",
    "            #find binary weights in the model\n",
    "            #Ensure that the weights are correctly changed before commiting to design. If assignment doesn't work I'll save the keys to access\n",
    "\n",
    "            #Saves the keys to access all the layers with binary weights as well as the weight statistics for that layer\n",
    "            binaryWeights = [] \n",
    "            totalCount = 0\n",
    "            for name1, _ in model.weightStatistics.items():\n",
    "                if name1==\"total\":\n",
    "                    continue\n",
    "                for name2, weightStats in model.weightStatistics[name1].items():\n",
    "                    if weightStats[\"numberOfUniqueValues\"]==2:\n",
    "                        if includeEncoding == False and name2==\"embeddings\":\n",
    "                            continue\n",
    "                        binaryWeights.append((name1, name2, weightStats))\n",
    "                        totalCount+=weightStats[\"totalValues\"]\n",
    "\n",
    "            if type(amount)==int:\n",
    "                #Randomly selects \"amount\" bits to flip\n",
    "                #The probability is equal for all applicable parameters\n",
    "                for i in range(amount):\n",
    "                    #Parameter used to figure out the index where flip happens\n",
    "                    index = np.random.randint(totalCount)\n",
    "                    for name1, name2, stats in binaryWeights:\n",
    "                        layerShape = model.model.params[name1][name2].shape\n",
    "                        layerCount = layerShape[0]*layerShape[1]\n",
    "\n",
    "                        #Check if this layer is the layer where the flip happens\n",
    "                        if index>=layerCount:   #Not the correct layer\n",
    "                            index-=layerCount\n",
    "                            continue\n",
    "                        \n",
    "                        #Flip happens on this layer\n",
    "                        index = (index//layerShape[1], index%layerShape[1])\n",
    "                        model.model.params[name1][name2] = model.model.params[name1][name2].at[index[0],index[1]].set(\n",
    "                            stats[\"maxValue\"] - float(model.model.params[name1][name2][index[0],index[1]])\n",
    "                        )\n",
    "\n",
    "                        #print(\"Flip at:\",name1,name2,index)\n",
    "\n",
    "                        break\n",
    "\n",
    "            elif type(amount)==float:\n",
    "                print(\"Percentage bitflip not yet implemented\")\n",
    "                return\n",
    "            else:\n",
    "                print(\"Error: amount needs to be int or float\")\n",
    "\n",
    "        case \"gaussian\":\n",
    "\n",
    "            if type(amount)==int:\n",
    "                print(\"Counted gaussian not yet implemented\")\n",
    "                return\n",
    "            \n",
    "            #Adds gaussian noise with standard deviation \"param\" to \"amount\" fraction of the weights\n",
    "            elif type(amount)==float:\n",
    "                for name1, _ in model.weightStatistics.items():\n",
    "                    if name1==\"total\":\n",
    "                        continue\n",
    "                    for name2, weightStats in model.weightStatistics[name1].items():\n",
    "                        if name2==\"b\":\n",
    "                            continue\n",
    "                        if includeEncoding == False and name2==\"embeddings\":\n",
    "                            continue\n",
    "\n",
    "                        layerShape = model.model.params[name1][name2].shape\n",
    "                        model.model.params[name1][name2] = model.model.params[name1][name2] + \\\n",
    "                            np.where(np.random.rand(layerShape[0], layerShape[1])<amount, np.random.normal(0, param, layerShape), 0)\n",
    "                        \n",
    "                return\n",
    "            else:\n",
    "                print(\"Error: amount needs to be int or float\")\n",
    "\n",
    "        case \"flipFirst\":\n",
    "            model.model.params[\"transformer/layer_0/attn/key\"][\"w\"] = model.model.params[\"transformer/layer_0/attn/key\"][\"w\"].at[0,0].set(1)\n",
    "            #weights = model.model.params[\"transformer/layer_0/attn/key\"][\"w\"]   #This assignment does not work. Guessing it copies due to strict immutability\n",
    "            #weights = weights.at[0,0].set(1)\n",
    "            print(model.model.params[\"transformer/layer_0/attn/key\"][\"w\"][0,0])\n",
    "            print(model.model.params[\"transformer/layer_0/attn/key\"][\"w\"])\n",
    "\n",
    "        case \"temp\":\n",
    "            shape = model.model.params[\"transformer/layer_0/attn/key\"][\"w\"].shape\n",
    "            print(shape)\n",
    "            model.model.params[\"transformer/layer_0/attn/key\"][\"w\"] = model.model.params[\"transformer/layer_0/attn/key\"][\"w\"]+np.ones(shape) #Easy lol\n",
    "            return\n",
    "\n",
    "\n",
    "        case _:\n",
    "            print(\"Error: noiseType not implemented\")\n",
    "\n",
    "    \n",
    "    return\n",
    "\n",
    "name = \"reverse\"\n",
    "maxSeqLen = 5\n",
    "model = generateModel(name, maxSeqLen)\n",
    "addNoise(model, noiseType=\"gaussian\", amount=1.0, param=0.001)\n",
    "model.updateWeightStatistics()\n",
    "model.printWeightStatistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: reverse\n",
      "Accuracy: 0.831\n"
     ]
    }
   ],
   "source": [
    "booleanAccuracy = evaluateModel(model, data)\n",
    "accuracy=np.mean(booleanAccuracy)\n",
    "print(\"Accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to figure out what kind of haiku model the tracr models are (how do they relate to pure haiku models) and how I can train these models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "#### Testing the base functions and generating the test data\n",
    "\n",
    "The sort function does not have a 100% accuracy. This only seems to apply when including input token 0, if only using 1 and up it seems to work. A cursory analysis would suggest that the min value in the sort function is multiplied with the indicies which makes it indistinguishable if the minimum value is 1.\n",
    "\n",
    "The most-freq function does not work in the same method as the original RASP paper (despite the Tracr paper claiming they recreated the RASP function in Tracr). Instead of backfilling with BOS tokens it simply sorts all tokens in groups. The most-freq function (make_sort_freq) is also hardcoded to only accept 1 as the min_key value for some reason. I could fix this but it is not really a high priority (and seemingly breaks the sort function)\n",
    "\n",
    "The most-freq function seems to fail sometimes (always?) when there are mutiple groups of the same count. Maybe they did not actually sort the output based on token grupings and only on frequency? Need to check. That apears to be the case. The Tracr make_sort_freq function is lazy and does not differentiate between tokens as long as the count is the same.\n",
    "\n",
    "Shuffle dyck \n",
    "* The RASP paper uses the tokens T, P and F to account for if a dyck-k sequence is legal, possible legal or not legal for each token in the sequence. The Tracr implementation on the other hand only uses 1 or 0 to show if the entire sequence is legal or not. This is a far simpler solution yet for some reason they explicitly claim that this is how it is implemented in the RASP paper in their code ???\n",
    "* If tokens are randomly selected most sequences will be unblanaced e.g. only even sequences can be balanced and if the sequence starts with a end token it wll be unblanced.\n",
    "* I should probably try to generate the sequence such that the probability of a balanced sequence is roughly 50%\n",
    "\n",
    "#### Analyzing weights\n",
    "\n",
    "What do I need to look out for? All of these should probably be applied layerwise (for each matrix of weights) and globally\n",
    "* Maximum/minimum values?\n",
    "* Binary values?\n",
    "* All same values?\n",
    "* Percentage which is 0?\n",
    "\n",
    "It seems like all of the \"b\" weights are zero vectors for the given models. As such I feel like I should mostly stay away from those vectors when adding noise and training\n",
    "\n",
    "Many of the layer weights are zero. The layer weights are usually binary or ternary, very rarely do the layer assume more values than 3.\n",
    "\n",
    "The percentage of values which are zero is usually between 90 and 100%.\n",
    "\n",
    "#### Adding noise\n",
    "\n",
    "Flipping a set amunt of bits will often have no effect. The influence of a flipped bit is heavily dependent on which bit is flipped. I cannot say what specific bits are highly influential though. This behaviour strikes me as odd since I would intuit that binary weights are done so for a reason, that is all weights should be relevant at some points.\n",
    "\n",
    "Adding gaussian noise seems to give a better range of failure. The failed percentage increases \"exponentially\"ish with how large the noise is unlike bitflips which can cause large errors or no difference by flipping a single bit.\n",
    "\n",
    "#### Training\n",
    "\n",
    "Haiku is needlessly complicated. E.g. generating new sequences requires manually updating the rng_key each time instead of doing it within the functions themselves. Everything is wrapped with mutplie layers of functions and classes which makes it very difficult to keep track of what is what"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RASP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

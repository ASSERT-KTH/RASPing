{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tracr.rasp import rasp\n",
    "from tracr.compiler import compiling\n",
    "from tracr.compiler import lib\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.model import Model\n",
    "from src.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prints the overleaf table body from a pandas dataframe\n",
    "def dataFrameToOverleafTableBody(df: pd.DataFrame, decimalNumber = 2):\n",
    "    columnCount = len(df.columns.values)\n",
    "    tableBody = \"\\\\begin{tabular}{|\"+ \"\".join([\"c|\" for i in range(columnCount)]) +\"} \\\\hline \\n\"\n",
    "    tableBody += \" & \".join(map(lambda x: \"\\\\textbf{\"+str(x)+\"}\", df.columns.values)) + \"\\\\\\\\ \\\\hline \\n\"\n",
    "    for _, row in df.iterrows():\n",
    "        if decimalNumber:\n",
    "            newRow = row.values[0] + \" & \"\n",
    "            newRow += \" & \".join(map(lambda x: \"%.2f\"%x, row.values[1:])) + \"\\\\\\\\ \\\\hline \\n\"\n",
    "        else:\n",
    "            newRow = \" & \".join(map(str, row.values)) + \"\\\\\\\\ \\\\hline \\n\"\n",
    "        tableBody += newRow\n",
    "    tableBody += \"\\\\end{tabular} \\n\"\n",
    "    return tableBody\n",
    "\n",
    "#Merge the results from the files in list. The files need to have the same dimension\n",
    "def loadArray(fileNames=\"temp\", baseDirectory = None):\n",
    "    if baseDirectory:\n",
    "        if type(fileNames) is list:\n",
    "            fileNames = [baseDirectory + name for name in fileNames]\n",
    "        else:\n",
    "            fileNames = baseDirectory + fileNames\n",
    "\n",
    "    if type(fileNames) is list and len(fileNames)==1:\n",
    "        fileNames = fileNames[0]\n",
    "\n",
    "    if type(fileNames) is list:\n",
    "        file = open(fileNames[0], \"rb\")\n",
    "        array = np.load(file).reshape((-1,1))\n",
    "        file.close()\n",
    "        for fn in fileNames[1:]:\n",
    "            file = open(fn, \"rb\")\n",
    "            conAcc = np.load(file).reshape((-1,1))\n",
    "            array = np.concatenate((array, conAcc), axis=1)\n",
    "            file.close()\n",
    "        \n",
    "    else:\n",
    "        file = open(fileNames, \"rb\")\n",
    "        array = np.load(file)\n",
    "        file.close()\n",
    "\n",
    "    return array\n",
    "\n",
    "#Returns all possible comnination of 'baseExpression' split at \"{}\" and filed with 'modificationsLists'\n",
    "def getListOfNames(baseExpression: str, modificationsLists: list):\n",
    "    listOfNames = []\n",
    "    splitBase = baseExpression.split(\"{}\")\n",
    "    if len(splitBase) != len(modificationsLists) + 1:\n",
    "        print(\"Error: Possible modifications (\"+str(len(splitBase)-1)+\") does not match given modifications (\"+str(len(modificationsLists))+\")\")\n",
    "        raise RuntimeError\n",
    "    \n",
    "    if len(splitBase) == 2:\n",
    "        for modification in modificationsLists[0]:\n",
    "            listOfNames.append(str(modification).join(splitBase))\n",
    "        return listOfNames\n",
    "    \n",
    "    for modification in modificationsLists[0]:\n",
    "        listOfNames += getListOfNames(splitBase[0] + str(modification) + \"{}\".join(splitBase[1:]), modificationsLists[1:])\n",
    "\n",
    "    return listOfNames\n",
    "\n",
    "#Creates and returns a list of names based on standard for each model test\n",
    "def createListOfNames(testName, baseDirectory = \"\", paramNames = None):\n",
    "    match testName:\n",
    "        case \"overTraining\":\n",
    "            if paramNames is None:\n",
    "                paramNames = [\"v1\", \"v2\", \"v3\"]\n",
    "            baseExpression = \"{}{}_{}_{}_{}\"\n",
    "            modificationsLists = [[\"random_\", \"\"],\n",
    "                                  [\"train\", \"val\"],\n",
    "                                  [\"sort\", \"reverse\", \"hist\", \"most-freq\", \"shuffle_dyck1\", \"shuffle_dyck2\"],\n",
    "                                  [\"loss\",\"acc\"],\n",
    "                                  paramNames]\n",
    "            nameList = getListOfNames(baseExpression, modificationsLists)\n",
    "            \n",
    "        case \"bitFlip\":\n",
    "            if paramNames is None:\n",
    "                paramNames = [\"0.0\", \"0.02\", \"0.04\", \"0.06\", \"0.08\", \"0.1\"]\n",
    "            baseExpression = \"{}_{}_{}_bitflip_fliprate_{}_{}\"\n",
    "            modificationsLists = [[\"sort\", \"reverse\", \"hist\", \"most-freq\", \"shuffle_dyck1\", \"shuffle_dyck2\"],\n",
    "                                 [\"train\", \"val\"],\n",
    "                                 [\"loss\", \"acc\"],\n",
    "                                 paramNames,\n",
    "                                 [\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "                                 ]\n",
    "            nameList = getListOfNames(baseExpression, modificationsLists)\n",
    "\n",
    "        case \"gaussian\":\n",
    "            if paramNames is None:\n",
    "                paramNames = [\"0.01\", \"0.25\", \"0.50\", \"0.75\", \"1.0\", \"1.25\"]\n",
    "            baseExpression = \"{}_{}_{}_gaussian_std{}_{}\"\n",
    "            modificationsLists = [[\"sort\", \"reverse\", \"hist\", \"most-freq\", \"shuffle_dyck1\", \"shuffle_dyck2\"],\n",
    "                                 [\"train\", \"val\"],\n",
    "                                 [\"loss\", \"acc\"],\n",
    "                                 paramNames,\n",
    "                                 [\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "                                 ]\n",
    "            nameList = getListOfNames(baseExpression, modificationsLists)\n",
    "\n",
    "        case \"mutated\":\n",
    "            if paramNames is None:\n",
    "                paramNames = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "            baseExpression = \"{}_{}_{}_{}\"\n",
    "            modificationsLists = [[\"sort\", \"reverse\", \"hist\", \"most-freq\", \"shuffle_dyck1\", \"shuffle_dyck2\"],\n",
    "                                 [\"train\", \"val\"],\n",
    "                                 [\"loss\", \"acc\"],\n",
    "                                 paramNames,\n",
    "                                 ]\n",
    "            nameList = getListOfNames(baseExpression, modificationsLists)\n",
    "\n",
    "        case _:\n",
    "            print(\"Error:\", testName, \"is not a valid test name\")\n",
    "        \n",
    "    nameList = [baseDirectory + name for name in nameList]\n",
    "\n",
    "    return nameList\n",
    "\n",
    "#Classify fileName according to model base and loss/acc\n",
    "def tagFileName(fileName):\n",
    "    tags = {}\n",
    "\n",
    "def dictAppend(dic, id, value):\n",
    "    if id in dic:\n",
    "        dic[id].append(value)\n",
    "    else:\n",
    "        dic[id]=[value]\n",
    "        \n",
    "def dataFrameHelper(dfDict, prefix, includedParameters, relevantFileNames, useAverage, useMean, useStd, baseDirectory = None):\n",
    "    for param in includedParameters:\n",
    "        paramFileNames = [name for name in relevantFileNames if str(param) in name]\n",
    "\n",
    "        #Start adding to the fields\n",
    "        print(param)\n",
    "        print(paramFileNames)\n",
    "        data = loadArray(paramFileNames, baseDirectory)\n",
    "\n",
    "        if useAverage:\n",
    "            if useMean:\n",
    "                dictAppend(dfDict, prefix+\"mean\"+str(param), data.mean(axis=1)[-1])\n",
    "            if useStd:\n",
    "                dictAppend(dfDict, prefix+\"std\"+str(param), data.std(axis=1)[-1])\n",
    "        else:\n",
    "            dictAppend(dfDict, prefix+str(param), data[-1])\n",
    "\n",
    "def createDataFrameFromFileNames(fileNames, includedParameters, useAverage = True, useMean = True, useStd = True, includeTrain = False, includeValidation = True, includeLoss = False,\n",
    "                                  includeAcc = True, rowIsModel = True, includeRandom = False, includeNonRandom = True, baseDirectory = None):\n",
    "    #Possible columns (All at certain checkpoints e.g. end, start, certain epoch)\n",
    "        #Mean, std\n",
    "\n",
    "    #Problem: Some experiments use mean others not (regular, grokking and mutations)\n",
    "        #Solution. If experiment uses mean it is consistent throughout, add as a parameter flag\n",
    "        #using average implies that we want to include mean and std. I'll add flag for min/max as well\n",
    "\n",
    "    #Problem: Currently only gives fileNames, I need model names extracted as well as loss/acc and train/validation\n",
    "    #This requires linking a model name, train/validation and loss/acc with corresponding fileNames\n",
    "        #Solution should be able to build a parser function which can classify each fileName by model name, loss/acc before extracting data \n",
    "        #Not good enough. I need to differentiate between model name, train/val, loss/acc and parameter value\n",
    "        #This suggests it should be easier to parse these from the baseExpression directly \n",
    "\n",
    "    dfDict = {}\n",
    "\n",
    "    #Sort results with row as model\n",
    "    if rowIsModel:\n",
    "        dfDict[\"Model\"]=[\"Sort\",\"Reverse\",\"Hist\",\"Most-Freq\",\"Dyck1\",\"Dyck2\"]\n",
    "\n",
    "        #Itterate trough all models\n",
    "        for modelName in [\"sort\", \"reverse\", \"hist\", \"most-freq\", \"dyck1\", \"dyck2\"]:\n",
    "            modelFileNames = [name for name in fileNames if modelName in name]\n",
    "\n",
    "            #Non random\n",
    "            if includeNonRandom:\n",
    "                nonRandomFileNames = [name for name in modelFileNames if \"random\" not in name]\n",
    "\n",
    "                if includeTrain:\n",
    "                    trainFileNames = [name for name in nonRandomFileNames if \"train\" in name]\n",
    "\n",
    "                    if includeAcc:\n",
    "                        accFileNames = [name for name in trainFileNames if \"acc\" in name]\n",
    "\n",
    "                        dataFrameHelper(dfDict, \"ta\", includedParameters, accFileNames, useAverage, useMean, useStd, baseDirectory)\n",
    "\n",
    "                    if includeLoss:\n",
    "                        lossFileNames = [name for name in trainFileNames if \"loss\" in name]\n",
    "\n",
    "                        dataFrameHelper(dfDict, \"tl\", includedParameters, lossFileNames, useAverage, useMean, useStd, baseDirectory)\n",
    "\n",
    "                if includeValidation:\n",
    "                    valFileNames = [name for name in nonRandomFileNames if \"val\" in name]\n",
    "\n",
    "                    if includeAcc:\n",
    "                        accFileNames = [name for name in valFileNames if \"acc\" in name]\n",
    "\n",
    "                        dataFrameHelper(dfDict, \"va\", includedParameters, accFileNames, useAverage, useMean, useStd, baseDirectory)\n",
    "\n",
    "                    if includeLoss:\n",
    "                        lossFileNames = [name for name in valFileNames if \"loss\" in name]\n",
    "\n",
    "                        dataFrameHelper(dfDict, \"vl\", includedParameters, lossFileNames, useAverage, useMean, useStd, baseDirectory)\n",
    "\n",
    "            #Random\n",
    "            if includeRandom:\n",
    "                randomFileNames = [name for name in modelFileNames if \"random\" in name]\n",
    "\n",
    "                if includeTrain:\n",
    "                    trainFileNames = [name for name in randomFileNames if \"train\" in name]\n",
    "\n",
    "                    if includeAcc:\n",
    "                        accFileNames = [name for name in trainFileNames if \"acc\" in name]\n",
    "\n",
    "                        dataFrameHelper(dfDict, \"randta\", includedParameters, accFileNames, useAverage, useMean, useStd, baseDirectory)\n",
    "\n",
    "                    if includeLoss:\n",
    "                        lossFileNames = [name for name in trainFileNames if \"loss\" in name]\n",
    "\n",
    "                        dataFrameHelper(dfDict, \"randtl\", includedParameters, lossFileNames, useAverage, useMean, useStd, baseDirectory)\n",
    "\n",
    "                if includeValidation:\n",
    "                    valFileNames = [name for name in randomFileNames if \"val\" in name]\n",
    "\n",
    "                    if includeAcc:\n",
    "                        accFileNames = [name for name in valFileNames if \"acc\" in name]\n",
    "\n",
    "                        dataFrameHelper(dfDict, \"randva\", includedParameters, accFileNames, useAverage, useMean, useStd, baseDirectory)\n",
    "\n",
    "                    if includeLoss:\n",
    "                        lossFileNames = [name for name in valFileNames if \"loss\" in name]\n",
    "\n",
    "                        dataFrameHelper(dfDict, \"randvl\", includedParameters, lossFileNames, useAverage, useMean, useStd, baseDirectory)\n",
    "            \n",
    "            \n",
    "                    \n",
    "    else:\n",
    "        print(\"Only model rows are implemented\")            \n",
    "\n",
    "    #Specially clause for regular overtraining results\n",
    "    if not (includeTrain or includeValidation):\n",
    "        print(\"Not yet implemented aka kinda useless at the moment\")\n",
    "        raise RuntimeError\n",
    "\n",
    "    return pd.DataFrame(dfDict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sort_loss_v1', 'sort_loss_v2', 'sort_loss_v3', 'reverse_loss_v1', 'reverse_loss_v2', 'reverse_loss_v3', 'hist_loss_v1', 'hist_loss_v2', 'hist_loss_v3', 'most-freq_loss_v1', 'most-freq_loss_v2', 'most-freq_loss_v3', 'dyck1_loss_v1', 'dyck1_loss_v2', 'dyck1_loss_v3', 'dyck2_loss_v1', 'dyck2_loss_v2', 'dyck2_loss_v3']\n"
     ]
    }
   ],
   "source": [
    "baseExpression = \"{}_{}_{}\"\n",
    "modificationsLists = [[\"sort\", \"reverse\", \"hist\", \"most-freq\", \"dyck1\", \"dyck2\"],\n",
    "                      [\"loss\", \"val\"],\n",
    "                      [\"v1\", \"v2\", \"v3\"]]\n",
    "\n",
    "fileNames = getListOfNames(baseExpression, modificationsLists)\n",
    "\n",
    "trainNames = [name for name in fileNames if \"loss\" in name]\n",
    "print(trainNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Gaussian model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{|c|c|c|c|c|c|c|} \\hline \n",
      "\\textbf{Model} & \\textbf{vamean0.01} & \\textbf{vamean0.25} & \\textbf{vamean0.50} & \\textbf{vamean0.75} & \\textbf{vamean1.0} & \\textbf{vamean1.25}\\\\ \\hline \n",
      "Sort & 0.95 & 0.99 & 0.99 & 0.97 & 1.00 & 0.99\\\\ \\hline \n",
      "Reverse & 1.00 & 0.91 & 1.00 & 0.93 & 0.88 & 0.68\\\\ \\hline \n",
      "Hist & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00\\\\ \\hline \n",
      "Most-Freq & 0.36 & 0.38 & 0.36 & 0.37 & 0.36 & 0.36\\\\ \\hline \n",
      "Dyck1 & 0.91 & 0.84 & 0.85 & 0.91 & 0.87 & 0.88\\\\ \\hline \n",
      "Dyck2 & 0.93 & 0.89 & 0.91 & 0.90 & 0.94 & 0.88\\\\ \\hline \n",
      "\\end{tabular} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseDirectory = os.path.abspath(os.path.join('../..')) + \"/PerformanceTesting/savedData/noiseTrainingGaussian2/\"\n",
    "fileNames = createListOfNames(\"gaussian\")\n",
    "df = createDataFrameFromFileNames(fileNames, includeTrain=False, useStd=False, useMean=True, includedParameters = [\"0.01\", \"0.25\", \"0.50\", \"0.75\", \"1.0\", \"1.25\"], baseDirectory=baseDirectory)\n",
    "print(dataFrameToOverleafTableBody(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Overtraining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{|c|c|c|c|c|c|c|} \\hline \n",
      "\\textbf{Model} & \\textbf{vav1} & \\textbf{vav2} & \\textbf{vav3} & \\textbf{randvav1} & \\textbf{randvav2} & \\textbf{randvav3}\\\\ \\hline \n",
      "Sort & 1.00 & 1.00 & 0.07 & 0.01 & 0.01 & 0.00\\\\ \\hline \n",
      "Reverse & 1.00 & 1.00 & 0.01 & 0.00 & 0.00 & 0.00\\\\ \\hline \n",
      "Hist & 1.00 & 1.00 & 1.00 & 0.05 & 0.01 & 0.00\\\\ \\hline \n",
      "Most-Freq & 0.82 & 0.32 & 0.21 & 0.00 & 0.00 & 0.00\\\\ \\hline \n",
      "Dyck1 & 1.00 & 0.98 & 0.98 & 1.00 & 0.73 & 0.84\\\\ \\hline \n",
      "Dyck2 & 0.95 & 0.75 & 0.91 & 0.95 & 0.84 & 0.81\\\\ \\hline \n",
      "\\end{tabular} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseDirectory = os.path.abspath(os.path.join('../..')) + \"/PerformanceTesting/savedData/overTrainingV2/\"\n",
    "fileNames = createListOfNames(\"overTraining\")\n",
    "df = createDataFrameFromFileNames(fileNames, includeTrain=False, useAverage=False, includeRandom=True, includedParameters = [\"v1\",\"v2\",\"v3\"], baseDirectory=baseDirectory)\n",
    "print(dataFrameToOverleafTableBody(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base bitFlip model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{|c|c|c|c|c|c|c|} \\hline \n",
      "\\textbf{Model} & \\textbf{vamean0.0} & \\textbf{vamean0.02} & \\textbf{vamean0.04} & \\textbf{vamean0.06} & \\textbf{vamean0.08} & \\textbf{vamean0.1}\\\\ \\hline \n",
      "Sort & 0.82 & 0.99 & 0.83 & 0.65 & 0.65 & 0.12\\\\ \\hline \n",
      "Reverse & 0.34 & 0.21 & 0.45 & 0.17 & 0.06 & 0.15\\\\ \\hline \n",
      "Hist & 0.26 & 0.25 & 0.03 & 0.02 & 0.02 & 0.01\\\\ \\hline \n",
      "Most-Freq & 0.14 & 0.20 & 0.04 & 0.04 & 0.05 & 0.06\\\\ \\hline \n",
      "Dyck1 & 0.88 & 0.85 & 0.93 & 0.84 & 0.83 & 0.84\\\\ \\hline \n",
      "Dyck2 & 0.90 & 0.94 & 0.96 & 0.87 & 0.83 & 0.86\\\\ \\hline \n",
      "\\end{tabular} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseDirectory = os.path.abspath(os.path.join('../..')) + \"/PerformanceTesting/savedData/noiseTrainingBitFlip/\"\n",
    "fileNames = createListOfNames(\"bitFlip\")\n",
    "df = createDataFrameFromFileNames(fileNames, includeTrain=False, useStd=False, useMean=True, includedParameters = [\"0.0\", \"0.02\", \"0.04\", \"0.06\", \"0.08\", \"0.1\"], baseDirectory=baseDirectory)\n",
    "print(dataFrameToOverleafTableBody(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base mutated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['sort_val_acc_0']\n",
      "1\n",
      "['sort_val_acc_1']\n",
      "2\n",
      "['sort_val_acc_2']\n",
      "3\n",
      "['sort_val_acc_3']\n",
      "4\n",
      "['sort_val_acc_4']\n",
      "0\n",
      "['reverse_val_acc_0']\n",
      "1\n",
      "['reverse_val_acc_1']\n",
      "2\n",
      "['reverse_val_acc_2']\n",
      "3\n",
      "['reverse_val_acc_3']\n",
      "4\n",
      "['reverse_val_acc_4']\n",
      "0\n",
      "['hist_val_acc_0']\n",
      "1\n",
      "['hist_val_acc_1']\n",
      "2\n",
      "['hist_val_acc_2']\n",
      "3\n",
      "['hist_val_acc_3']\n",
      "4\n",
      "['hist_val_acc_4']\n",
      "0\n",
      "['most-freq_val_acc_0']\n",
      "1\n",
      "['most-freq_val_acc_1']\n",
      "2\n",
      "['most-freq_val_acc_2']\n",
      "3\n",
      "['most-freq_val_acc_3']\n",
      "4\n",
      "['most-freq_val_acc_4']\n",
      "0\n",
      "['shuffle_dyck1_val_acc_0']\n",
      "1\n",
      "['shuffle_dyck1_val_acc_0', 'shuffle_dyck1_val_acc_1', 'shuffle_dyck1_val_acc_2', 'shuffle_dyck1_val_acc_3', 'shuffle_dyck1_val_acc_4']\n",
      "2\n",
      "['shuffle_dyck1_val_acc_2']\n",
      "3\n",
      "['shuffle_dyck1_val_acc_3']\n",
      "4\n",
      "['shuffle_dyck1_val_acc_4']\n",
      "0\n",
      "['shuffle_dyck2_val_acc_0']\n",
      "1\n",
      "['shuffle_dyck2_val_acc_1']\n",
      "2\n",
      "['shuffle_dyck2_val_acc_0', 'shuffle_dyck2_val_acc_1', 'shuffle_dyck2_val_acc_2', 'shuffle_dyck2_val_acc_3', 'shuffle_dyck2_val_acc_4']\n",
      "3\n",
      "['shuffle_dyck2_val_acc_3']\n",
      "4\n",
      "['shuffle_dyck2_val_acc_4']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only length-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[169], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m fileNames \u001b[38;5;241m=\u001b[39m createListOfNames(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmutated\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m createDataFrameFromFileNames(fileNames, includeTrain\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, useAverage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, includedParameters \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4\u001b[39m\u001b[38;5;124m\"\u001b[39m], baseDirectory\u001b[38;5;241m=\u001b[39mbaseDirectory)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdataFrameToOverleafTableBody\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[168], line 9\u001b[0m, in \u001b[0;36mdataFrameToOverleafTableBody\u001b[1;34m(df, decimalNumber)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decimalNumber:\n\u001b[0;32m      8\u001b[0m     newRow \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m & \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m     newRow \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m & \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m%.2f\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mhline \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     newRow \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m & \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, row\u001b[38;5;241m.\u001b[39mvalues)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mhline \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[168], line 9\u001b[0m, in \u001b[0;36mdataFrameToOverleafTableBody.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decimalNumber:\n\u001b[0;32m      8\u001b[0m     newRow \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m & \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m     newRow \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m & \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m%.2f\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43mx\u001b[49m, row\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m1\u001b[39m:])) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mhline \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     newRow \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m & \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, row\u001b[38;5;241m.\u001b[39mvalues)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mhline \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "baseDirectory = os.path.abspath(os.path.join('../..')) + \"/PerformanceTesting/savedData/mutatedModels1/\"\n",
    "fileNames = createListOfNames(\"mutated\")\n",
    "df = createDataFrameFromFileNames(fileNames, includeTrain=False, useAverage=False, includedParameters = [\"0\",\"1\",\"2\",\"3\",\"4\"], baseDirectory=baseDirectory)\n",
    "print(dataFrameToOverleafTableBody(df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
